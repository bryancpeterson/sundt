This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2025-04-10T21:26:36.456Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
crawlers/
  awards_crawler.py
  projects_crawler.py
.gitignore
awards_agent.py
projects_agent.py
requirements.txt
search_engine.py
sundt_cli.py
test_search_engine.py

================================================================
Repository Files
================================================================

================
File: crawlers/awards_crawler.py
================
import os
import json
import requests
from bs4 import BeautifulSoup
import time
import re


class SundtAwardsCrawler:
    # Base URLs for the crawler
    BASE_URL = "https://www.sundt.com"
    AWARDS_URL = f"{BASE_URL}/about-us/awards-recognition/"

    def __init__(self, output_file="data/awards.json"):
        self.output_file = output_file
        # Create output directory if it doesn't exist
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        # Setup request headers to mimic a browser
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Content-Type": "application/json",
            "Accept": "application/json",
            "Referer": self.AWARDS_URL
        }

    def crawl(self):
        print(f"Crawling awards from {self.AWARDS_URL}")
        awards = []

        try:
            # Step 1: Get the main awards page
            response = requests.get(self.AWARDS_URL)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            
            # Step 2: Extract "New & Noteworthy" awards
            noteworthy_awards = self._extract_noteworthy_awards(soup)
            if noteworthy_awards:
                awards.extend(noteworthy_awards)
                print(f"Extracted {len(noteworthy_awards)} noteworthy awards")
            
            # Step 3: Extract "Additional Honors" awards (visible + hidden)
            additional_awards = self._extract_additional_honors(soup)
            if additional_awards:
                awards.extend(additional_awards)
                print(f"Extracted {len(additional_awards)} additional honors")
            
            # Step 4: Extract "News & Updates" awards
            news_awards = self._extract_news_updates(soup)
            if news_awards:
                awards.extend(news_awards)
                print(f"Extracted {len(news_awards)} news and updates")

        except Exception as e:
            print(f"Error crawling awards: {e}")

        # Save all data
        self._save_data(awards)
        print(f"Final count: {len(awards)} awards crawled")
        return awards

    def _extract_noteworthy_awards(self, soup):
        """Extract awards from the 'New & Noteworthy' section"""
        noteworthy_awards = []
        try:
            # Find the "New & Noteworthy" section
            noteworthy_section = soup.find("h4", class_="title-serif", string="New & Noteworthy")
            if not noteworthy_section:
                return noteworthy_awards
            
            # Find the container with the items
            items_container = noteworthy_section.find_parent("div", class_="section--cards")
            if not items_container:
                return noteworthy_awards
            
            # Find all award items
            award_items = items_container.select(".item")
            
            for item in award_items:
                # Extract title (header)
                title_elem = item.select_one(".item__head h3")
                title = title_elem.get_text(strip=True) if title_elem else ""
                
                # Extract description (body)
                desc_elem = item.select_one(".item__body p")
                description = desc_elem.get_text(strip=True) if desc_elem else ""
                
                # Extract icon/image if available
                img_elem = item.select_one(".item__image")
                image_url = ""
                if img_elem and "background-image" in img_elem.get("style", ""):
                    # Extract URL from background-image: url(...)
                    bg_img_match = re.search(r'url\((.*?)\)', img_elem.get("style", ""))
                    if bg_img_match:
                        image_url = bg_img_match.group(1).strip('\'"')
                        # Make relative URLs absolute
                        if image_url and not image_url.startswith(('http://', 'https://')):
                            image_url = self.BASE_URL + image_url if not image_url.startswith('/') else self.BASE_URL + image_url
                
                # Create award object
                award = {
                    "category": "New & Noteworthy",
                    "title": title,
                    "description": description,
                    "image_url": image_url
                }
                
                # Parse out award organization and date if possible
                if description:
                    # Try to extract organization and date from description
                    parts = description.split('/')
                    if len(parts) == 2:
                        award["organization"] = parts[0].strip()
                        award["date"] = parts[1].strip()
                    # If only one part, check if it's a date
                    elif len(parts) == 1 and re.search(r'\b(January|February|March|April|May|June|July|August|September|October|November|December)\b', description):
                        award["date"] = description.strip()
                
                noteworthy_awards.append(award)
                
        except Exception as e:
            print(f"Error extracting noteworthy awards: {e}")
            
        return noteworthy_awards

    def _extract_additional_honors(self, soup):
        """Extract awards from the 'Additional Honors' section, including hidden awards"""
        additional_awards = []
        try:
            # Find the "Additional Honors" section
            honors_section = soup.find("h5", class_="section__sub-title", string="Additional Honors")
            if not honors_section:
                return additional_awards
            
            # Find the container with the columns
            columns_container = honors_section.find_parent("div", class_="awardsSection")
            if not columns_container:
                return additional_awards
            
            # Process visible and hidden awards from both columns
            for column in columns_container.select(".col"):
                # Process visible awards in this column
                visible_awards = self._parse_awards_from_column(column, exclude_hidden=True)
                additional_awards.extend(visible_awards)
                
                # Process hidden awards in this column (those in div with class "hidden")
                hidden_div = column.select_one(".hidden")
                if hidden_div:
                    hidden_awards = self._parse_awards_from_column(hidden_div, exclude_hidden=False)
                    additional_awards.extend(hidden_awards)
                
        except Exception as e:
            print(f"Error extracting additional honors: {e}")
            
        return additional_awards
    
    def _parse_awards_from_column(self, column, exclude_hidden=False):
        """Parse award entries from a column in the Additional Honors section"""
        awards = []
        
        # Skip processing if this is a hidden div and we want to exclude hidden content
        if exclude_hidden and "hidden" in column.get("class", []):
            return awards
        
        # Group content by award entries
        # Each award typically has an h6 header followed by p tags
        current_award = None
        
        for elem in column.find_all(["h6", "p"]):
            # Skip if this element is within a hidden div and we want to exclude hidden
            if exclude_hidden and elem.find_parent(class_="hidden"):
                continue
                
            if elem.name == "h6":
                # Save previous award if exists
                if current_award:
                    awards.append(current_award)
                
                # Start a new award
                current_award = {
                    "category": "Additional Honors",
                    "award_type": elem.get_text(strip=True).replace("<em>", "").replace("</em>", "")
                }
            elif elem.name == "p" and current_award:
                # Add content to the current award
                content = elem.get_text(strip=True)
                
                # First p tag after h6 is usually the organization
                if "organization" not in current_award:
                    strong_elem = elem.find("strong")
                    if strong_elem:
                        current_award["organization"] = strong_elem.get_text(strip=True)
                        
                        # Remove organization from content for description
                        content = content.replace(current_award["organization"], "").strip()
                    
                # Check if there's a project link
                link_elem = elem.find("a")
                if link_elem:
                    project_url = link_elem.get("href")
                    project_title = link_elem.get_text(strip=True)
                    
                    if "projects" not in current_award:
                        current_award["projects"] = []
                    
                    current_award["projects"].append({
                        "title": project_title,
                        "url": project_url if project_url.startswith(('http://', 'https://')) else self.BASE_URL + project_url if not project_url.startswith('/') else self.BASE_URL + project_url
                    })
                
                # Add remaining content as description
                if content and content != current_award.get("organization", ""):
                    if "description" not in current_award:
                        current_award["description"] = content
                    else:
                        current_award["description"] += " " + content
        
        # Don't forget to add the last award
        if current_award:
            awards.append(current_award)
            
        # Process descriptions to extract location and year
        for award in awards:
            if "description" in award:
                # Try to extract location (usually at the end before the year)
                location_match = re.search(r'([A-Za-z\s]+),\s*([A-Z]{2})\s*\d{4}', award["description"])
                if location_match:
                    award["location"] = f"{location_match.group(1)}, {location_match.group(2)}".strip()
                
                # Try to extract year (usually a 4-digit number at the end)
                year_match = re.search(r'\b(20\d{2})\b', award["description"])
                if year_match:
                    award["year"] = year_match.group(1)
        
        return awards

    def _extract_news_updates(self, soup):
        """Extract awards from the 'News & Updates' section"""
        news_awards = []
        try:
            # Find the "News & Updates" section
            news_section = soup.find("h5", class_="section__sub-title", string="News & Updates")
            if not news_section:
                return news_awards
            
            # Find the container with the slider
            slider_container = news_section.find_parent("div", class_="section--card-slider")
            if not slider_container:
                return news_awards
            
            # Find all slides
            slides = slider_container.select(".slider__slide")
            
            for slide in slides:
                # Extract card content
                card = slide.select_one(".card")
                if not card:
                    continue
                
                # Extract link
                link_elem = card.select_one("a")
                link = link_elem.get("href") if link_elem else ""
                if link and not link.startswith(('http://', 'https://')):
                    link = self.BASE_URL + link if not link.startswith('/') else self.BASE_URL + link
                
                # Extract title
                title_elem = card.select_one("h4")
                title = title_elem.get_text(strip=True) if title_elem else ""
                
                # Extract description
                desc_elem = card.select_one(".card-body")
                description = ""
                if desc_elem:
                    # Get text excluding the title
                    desc_text = desc_elem.get_text(strip=True)
                    if title in desc_text:
                        description = desc_text.replace(title, "").strip()
                    else:
                        description = desc_text
                
                # Extract image
                img_elem = card.select_one(".card-image")
                image_url = ""
                if img_elem and "background-image" in img_elem.get("style", ""):
                    # Extract URL from background-image: url(...)
                    bg_img_match = re.search(r'url\((.*?)\)', img_elem.get("style", ""))
                    if bg_img_match:
                        image_url = bg_img_match.group(1).strip('\'"')
                        # Make relative URLs absolute
                        if image_url and not image_url.startswith(('http://', 'https://')):
                            image_url = self.BASE_URL + image_url if not image_url.startswith('/') else self.BASE_URL + image_url
                
                # Create news award object
                news_award = {
                    "category": "News & Updates",
                    "title": title,
                    "description": description,
                    "url": link,
                    "image_url": image_url
                }
                
                news_awards.append(news_award)
                
        except Exception as e:
            print(f"Error extracting news updates: {e}")
            
        return news_awards

    def _save_data(self, awards):
        """Save the extracted award data to a JSON file."""
        try:
            # Process the awards to create a well-structured output
            processed_awards = process_awards(awards)
            with open(self.output_file, "w", encoding="utf-8") as f:
                json.dump({"awards": processed_awards}, f, indent=2)
            print(f"Saved {len(processed_awards)} awards to {self.output_file}")
        except Exception as e:
            print(f"Error saving data: {e}")

def process_awards(awards):
    """Process the crawled awards to create a well-structured final output."""
    processed_awards = []

    for award in awards:
        # Start with a clean award object
        processed_award = {
            "category": award.get("category", ""),
            "title": award.get("title", award.get("award_type", ""))
        }
        
        # Add organization if available
        if "organization" in award:
            processed_award["organization"] = award["organization"]
            
        # Add description if available
        if "description" in award:
            processed_award["description"] = award["description"]
            
        # Add location if available
        if "location" in award:
            processed_award["location"] = award["location"]
            
        # Add date/year if available
        if "date" in award:
            processed_award["date"] = award["date"]
        elif "year" in award:
            processed_award["year"] = award["year"]
            
        # Add image if available
        if "image_url" in award and award["image_url"]:
            processed_award["image_url"] = award["image_url"]
            
        # Add URL if available
        if "url" in award and award["url"]:
            processed_award["url"] = award["url"]
            
        # Add projects if available
        if "projects" in award and award["projects"]:
            processed_award["projects"] = award["projects"]
        
        processed_awards.append(processed_award)

    return processed_awards

if __name__ == "__main__":
    crawler = SundtAwardsCrawler()
    awards = crawler.crawl()
    print(f"Extracted {len(awards)} awards in total")

================
File: crawlers/projects_crawler.py
================
import os
import json
import requests
from bs4 import BeautifulSoup
import time
import re

class SundtProjectsCrawler:
    # Base URLs for the crawler
    BASE_URL = "https://www.sundt.com"
    PROJECTS_URL = f"{BASE_URL}/projects/"
    AJAX_URL = f"{BASE_URL}/wp-json/facetwp/v1/refresh"  # FacetWP AJAX endpoint

    def __init__(self, output_file="data/projects.json"):
        self.output_file = output_file
        # Create output directory if it doesn't exist
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        # Setup request headers to mimic a browser
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Content-Type": "application/json",
            "Accept": "application/json",
            "Referer": self.PROJECTS_URL  # Important for AJAX requests
        }
        # Track project URLs we've already seen to avoid duplicates
        self.seen_project_urls = set()

    def crawl(self):
        print(f"Crawling projects from {self.PROJECTS_URL}")
        projects = []

        try:
            # Step 1: Get the main projects page
            response = requests.get(self.PROJECTS_URL)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            
            # Extract projects from the first page
            project_cards = soup.select(".project-card")
            print(f"Processing {len(project_cards)} projects from first page")

            for card in project_cards:
                project_data = self._extract_project_data(card)
                if project_data and project_data['url'] not in self.seen_project_urls:
                    self.seen_project_urls.add(project_data['url'])
                    projects.append(project_data)

            # Step 2: Extract FacetWP settings for pagination
            fwp_settings = self._extract_fwp_settings(response.text)

            # Step 3: Handle pagination based on whether we found FacetWP settings
            if not fwp_settings:
                # Fallback: Try direct pagination if FacetWP settings couldn't be extracted
                print("Could not extract FacetWP settings. Will try direct pagination approach.")
                page = 2
                max_attempts = 20  # Safety limit to prevent infinite loops

                while page <= max_attempts:
                    print(f"Trying direct page {page}...")
                    page_url = f"{self.PROJECTS_URL}page/{page}/"
                    try:
                        page_response = requests.get(page_url)
                        if page_response.status_code != 200:
                            print(f"Reached end of pagination at page {page}")
                            break

                        page_soup = BeautifulSoup(page_response.text, "html.parser")
                        page_cards = page_soup.select(".project-card")
                        if not page_cards:
                            print(f"No project cards found on page {page}")
                            break

                        print(f"Processing {len(page_cards)} projects from page {page}")
                        for card in page_cards:
                            project_data = self._extract_project_data(card)
                            if project_data and project_data['url'] not in self.seen_project_urls:
                                self.seen_project_urls.add(project_data['url'])
                                projects.append(project_data)

                        page += 1
                        time.sleep(1)  # Be nice to the server
                    except Exception as e:
                        print(f"Error loading page {page}: {e}")
                        break
            else:
                # Use AJAX pagination with FacetWP settings
                total_pages = fwp_settings.get('total_pages', 20)  # Default to 20 if not found
                print(f"Found FacetWP settings. Total pages estimated: {total_pages}")

                # Process remaining pages using AJAX
                for page in range(2, total_pages + 1):
                    print(f"Loading page {page} of {total_pages} via AJAX")
                    
                    # Create payload with correct FacetWP structure
                    payload = {
                        "action": "facetwp_refresh",
                        "data": {
                            "extras": {"selections": True, "sort": "default"},
                            "facets": {
                                "region": [], "project_delivery_methods": [],
                                "project_markets": [], "project_submarket": [], "count": []
                            },
                            "first_load": 0,
                            "frozen_facets": {},
                            "http_params": {"get": [], "uri": "projects", "url_vars": []},
                            "is_bfcache": 1,
                            "paged": page,  # Current page number
                            "soft_refresh": 1,
                            "template": "projects"
                        }
                    }

                    try:
                        # Make AJAX request to load more projects
                        ajax_response = requests.post(
                            self.AJAX_URL,
                            headers=self.headers,
                            json=payload
                        )

                        if ajax_response.status_code != 200:
                            print(f"Error loading page {page}: Status {ajax_response.status_code}")
                            print(f"Response: {ajax_response.text[:200]}...")  # Print start of response
                            continue

                        try:
                            # Parse AJAX response
                            ajax_data = ajax_response.json()
                            
                            # Check different possible response structures
                            html_content = (
                                ajax_data.get('template') or
                                ajax_data.get('html') or
                                ajax_data.get('content')
                            )

                            if not html_content:
                                print(f"No HTML content returned for page {page}")
                                print(f"Response keys: {ajax_data.keys()}")
                                continue

                            # Extract projects from AJAX response
                            ajax_soup = BeautifulSoup(html_content, "html.parser")
                            ajax_project_cards = ajax_soup.select(".project-card")
                            if not ajax_project_cards:
                                print(f"No project cards found in AJAX response for page {page}")
                                continue

                            print(f"Processing {len(ajax_project_cards)} projects from page {page}")
                            for card in ajax_project_cards:
                                project_data = self._extract_project_data(card)
                                if project_data and project_data['url'] not in self.seen_project_urls:
                                    self.seen_project_urls.add(project_data['url'])
                                    projects.append(project_data)
                        except json.JSONDecodeError as e:
                            print(f"Error decoding JSON from AJAX response: {e}")
                            print(f"Response text: {ajax_response.text[:100]}...")
                    except Exception as e:
                        print(f"Error making AJAX request: {e}")

                    time.sleep(1)  # Be nice to the server

            # Step 4: Get detailed information for each project
            print(f"Getting detailed information for {len(projects)} projects...")
            for i, project in enumerate(projects):
                if 'url' in project and project['url']:
                    print(f"Processing project {i+1}/{len(projects)}: {project['title']}")
                    detailed_data = self._get_project_details(project['url'])
                    if detailed_data:
                        project.update(detailed_data)

                # Save intermediate results every 10 projects
                if (i + 1) % 10 == 0:
                    print(f"Processed {i + 1}/{len(projects)} project details")
                    self._save_data(projects)  # Checkpoint save

                time.sleep(0.5)  # Be nice to the server

        except Exception as e:
            print(f"Error crawling projects: {e}")

        # Final save of all data
        self._save_data(projects)
        print(f"Final count: {len(projects)} unique projects crawled")
        return projects

    def _extract_fwp_settings(self, html_content):
        """Extract FacetWP pagination settings from the HTML."""
        try:
            # Try multiple regex patterns to find FacetWP settings
            patterns = [
                r"window\.FWP_JSON\s*=\s*(\{.*?\});.*?window\.FWP_HTTP",
                r"var\s+FWP_JSON\s*=\s*(\{.*?\});",
                r"FWP\.preload_data\s*=\s*(\{.*?\});"
            ]

            for pattern in patterns:
                matches = re.search(pattern, html_content, re.DOTALL)
                if matches:
                    json_str = matches.group(1)
                    try:
                        data = json.loads(json_str)
                        # Try different JSON structures to find pagination info
                        if 'preload_data' in data and 'settings' in data['preload_data']:
                            return data['preload_data']['settings'].get('pager')
                        elif 'settings' in data and 'pager' in data['settings']:
                            return data['settings']['pager']
                        return data
                    except json.JSONDecodeError:
                        continue

            # Fallback: Look for total_pages directly in the HTML
            total_pages_match = re.search(r'total_pages["\']?\s*:\s*(\d+)', html_content)
            if total_pages_match:
                return {"total_pages": int(total_pages_match.group(1))}

            # Fallback: Look for pagination elements in the HTML
            soup = BeautifulSoup(html_content, 'html.parser')
            pagination = soup.select('.pagination, .nav-links, .facetwp-pager')
            if pagination:
                page_links = pagination[0].select('a')
                highest_page = max([
                    int(link.get_text(strip=True))
                    for link in page_links
                    if link.get_text(strip=True).isdigit()
                ], default=1)
                return {"total_pages": highest_page}
        except Exception as e:
            print(f"Error extracting FacetWP settings: {e}")

        return None

    def _extract_project_data(self, card):
        """Extract basic project data from a project card element."""
        try:
            # Find the link element (multiple possible selectors)
            link_element = card.select_one("a.project-card__link, a.card-link, .project-card a, .project a")
            if not link_element:
                return None

            # Get the project URL and ensure it's absolute
            project_url = link_element.get("href")
            if project_url and not project_url.startswith(('http://', 'https://')):
                project_url = self.BASE_URL + project_url if not project_url.startswith('/') else self.BASE_URL + project_url

            # Get the project title
            title_element = card.select_one(".project-card__title, .card-title, h2, h3, .title")
            title_text = title_element.get_text(strip=True) if title_element else "Unknown Project"
            title = re.sub(r'▶|→|⇒', '', title_text).strip()  # Remove arrow symbols

            # Get the project image
            img_element = card.select_one(".project-card__image img, .card-img img, img")
            image_url = img_element.get("src") if img_element else ""
            if image_url and not image_url.startswith(('http://', 'https://')):
                image_url = self.BASE_URL + image_url if not image_url.startswith('/') else self.BASE_URL + image_url

            # Get any snippet/excerpt text
            snippet_element = card.select_one(".project-card__excerpt, .card-text, .excerpt, p")
            snippet = snippet_element.get_text(strip=True) if snippet_element else ""

            return {
                "title": title,
                "url": project_url,
                "image_url": image_url,
                "snippet": snippet
            }
        except Exception as e:
            print(f"Error extracting project data from card: {e}")
            return None

    def _get_project_details(self, project_url):
        """Get detailed project information from the project page."""
        try:
            full_url = project_url if project_url.startswith("http") else self.BASE_URL + project_url
            response = requests.get(full_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            # Initialize the detailed data structure
            detailed_data = {
                "metadata": {},  # Will hold location, client, value, etc.
                "impact": {},    # Will hold Community Impact section
                "overview": "",  # Will hold Project Overview text
                "features": []   # Will hold Features & Highlights
            }

            # Extract metadata from the list-info section
            metadata_items = soup.select(".list-info li")
            for item in metadata_items:
                label_elem = item.select_one("h5")
                value_elem = item.select_one("p")
                if label_elem and value_elem:
                    # Clean up the label for use as a key
                    label = label_elem.get_text(strip=True).rstrip(':').lower().replace(' ', '_')
                    value = value_elem.get_text(strip=True)
                    
                    # Handle specialties as a list (comma-separated)
                    if label == "specialties":
                        detailed_data["metadata"][label] = [s.strip() for s in value.split(',')]
                    else:
                        detailed_data["metadata"][label] = value

            # Extract the Community Impact section
            impact_section = soup.find("h5", string=lambda text: text and "Community Impact" in text)
            if impact_section:
                impact_container = impact_section.find_parent("div", class_="ModalContainer")
                if impact_container:
                    # Get the impact title
                    impact_title = impact_container.select_one("h3")
                    if impact_title:
                        detailed_data["impact"]["title"] = impact_title.get_text(strip=True)
                    
                    # Get the impact description
                    impact_desc = impact_container.select_one("p")
                    if impact_desc:
                        detailed_data["impact"]["description"] = impact_desc.get_text(strip=True)

            # Extract Project Overview
            overview_section = soup.find("h6", string=lambda text: text and "Project Overview" in text)
            if overview_section:
                overview_container = overview_section.find_parent("div", class_="section__content")
                if overview_container:
                    # Get all paragraphs in the overview section
                    overview_paragraphs = overview_container.select("p:not([id])")
                    overview_text = " ".join([p.get_text(strip=True) for p in overview_paragraphs])
                    
                    # Also get any collapsed content (Read More sections)
                    collapsed_content = overview_container.select(".content.collapse p")
                    if collapsed_content:
                        overview_text += " " + " ".join([p.get_text(strip=True) for p in collapsed_content])
                    
                    detailed_data["overview"] = overview_text

            # Extract Features & Highlights
            features_section = soup.find("h3", string=lambda text: text and "Features & Highlights" in text)
            if features_section:
                features_container = features_section.find_parent("div", class_="section__aside")
                if features_container:
                    # Get all bullet points
                    feature_items = features_container.select("ul.list-bullets li")
                    detailed_data["features"] = [item.get_text(strip=True) for item in feature_items]

            # Extract any testimonial/quote
            blockquote = soup.select_one("blockquote")
            if blockquote:
                quote_text = blockquote.get_text(strip=True)
                if quote_text:
                    detailed_data["testimonial"] = quote_text

            return detailed_data
        except Exception as e:
            print(f"Error getting project details from {project_url}: {e}")
            return {}

    def _save_data(self, projects):
        """Save the extracted project data to a JSON file."""
        try:
            # Process the projects to create a well-structured output
            processed_projects = process_projects(projects)
            with open(self.output_file, "w", encoding="utf-8") as f:
                json.dump({"projects": processed_projects}, f, indent=2)
            print(f"Saved {len(processed_projects)} projects to {self.output_file}")
        except Exception as e:
            print(f"Error saving data: {e}")

def process_projects(projects):
    """Process the crawled projects to create a well-structured final output."""
    processed_projects = []

    for project in projects:
        # Start with basic project info
        processed_project = {
            "title": project.get("title", ""),
            "url": project.get("url", ""),
            "image_url": project.get("image_url", "")
        }

        # Add metadata as top-level fields
        if "metadata" in project:
            for key, value in project["metadata"].items():
                processed_project[key] = value

        # Add overview as description
        if "overview" in project and project["overview"]:
            processed_project["description"] = project["overview"]

        # Add impact section if available
        if "impact" in project and project["impact"]:
            processed_project["impact"] = project["impact"]

        # Add features if available
        if "features" in project and project["features"]:
            processed_project["features"] = project["features"]

        # Add testimonial if available
        if "testimonial" in project:
            processed_project["testimonial"] = project["testimonial"]

        processed_projects.append(processed_project)

    return processed_projects

if __name__ == "__main__":
    crawler = SundtProjectsCrawler()
    projects = crawler.crawl()
    print(f"Extracted {len(projects)} projects in total")

================
File: .gitignore
================
###############################################################################
#  General Python
###############################################################################
# Byte‑compiled / optimized files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

###############################################################################
#  Virtual Environments
###############################################################################
# (adjust names if you use a different folder)
venv/
env/
ENV/
.venv/
.conda/

###############################################################################
#  Packaging / Build artifacts
###############################################################################
build/
dist/
*.egg-info/
.eggs/
*.egg
pip-wheel-metadata/
*.whl

###############################################################################
#  Testing / Coverage
###############################################################################
.pytest_cache/
.nox/
.tox/
.coverage
coverage.xml
nosetests.xml
htmlcov/
.hypothesis/

###############################################################################
#  Jupyter
###############################################################################
.ipynb_checkpoints/

###############################################################################
#  IDE / Editor folders
###############################################################################
.vscode/
.idea/

###############################################################################
#  OS‑specific
###############################################################################
.DS_Store
Thumbs.db

###############################################################################
#  Project‑specific exclusions
###############################################################################
# Environment variables
.env

# Raw or generated data you don’t want in Git
data/

================
File: awards_agent.py
================
import os
import json
import re
from datetime import datetime
import time
from typing import Dict, Any, List
from search_engine import SearchEngine
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from dotenv import load_dotenv

# Load environment variables from .env file (contains OPENAI_API_KEY)
load_dotenv()

class AwardsAgent:
    """Agent specialized for retrieving award information"""
    
    def __init__(self, model_name="gpt-3.5-turbo", temperature=0.2):
        # Initialize search engine
        self.search_engine = SearchEngine()
        
        # Initialize OpenAI LLM
        self.model_name = model_name
        self.temperature = temperature
        self.llm = ChatOpenAI(model_name=model_name, temperature=temperature)
        
        # Set up the prompt template
        self.prompt_template = PromptTemplate(
            input_variables=["query", "award_data"],
            template="""
            You are the Awards Agent for Sundt Construction. Your role is to provide 
            accurate information about awards and recognition received by Sundt.
            
            USER QUERY: {query}
            
            AWARD DATA:
            {award_data}
            
            Based on the information provided, respond to the user's query about Sundt's awards.
            Present the information in a clear, concise, and helpful manner.
            If the provided data doesn't contain relevant information to answer the query,
            say that you don't have that specific information about Sundt's awards.
            
            RESPONSE:
            """
        )
        
        # Create the chain
        self.chain = LLMChain(llm=self.llm, prompt=self.prompt_template)
        
        # Set up metrics tracking
        self.metrics_file = os.path.join("data", "awards_agent_metrics.json")
        self.metrics = self._load_metrics()
    
    def _load_metrics(self) -> Dict[str, Any]:
        """Load metrics from file or initialize if not exists"""
        if os.path.exists(self.metrics_file):
            try:
                with open(self.metrics_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return self._initialize_metrics()
        else:
            return self._initialize_metrics()
    
    def _initialize_metrics(self) -> Dict[str, Any]:
        """Initialize empty metrics structure"""
        return {
            "total_queries": 0,
            "query_times": [],
            "injection_attempts": [],
            "queries_by_date": {},
            "popular_terms": {}
        }
    
    def _update_metrics(self, query: str, execution_time: float, 
                       is_injection: bool = False) -> None:
        """Update metrics with query information"""
        # Load latest metrics
        self.metrics = self._load_metrics()
        
        # Get today's date as string
        today = datetime.now().strftime("%Y-%m-%d")
        
        # Update total queries
        self.metrics["total_queries"] += 1
        
        # Update query times (keep the last 100)
        self.metrics["query_times"].append(execution_time)
        if len(self.metrics["query_times"]) > 100:
            self.metrics["query_times"] = self.metrics["query_times"][-100:]
        
        # Log injection attempts
        if is_injection:
            self.metrics["injection_attempts"].append({
                "query": query,
                "date": today
            })
        
        # Update date-based metrics
        if today not in self.metrics["queries_by_date"]:
            self.metrics["queries_by_date"][today] = 0
        self.metrics["queries_by_date"][today] += 1
        
        # Update popular terms (simple word frequency)
        words = re.findall(r'\b\w+\b', query.lower())
        for word in words:
            if len(word) > 3:  # Only count words with more than 3 characters
                if word not in self.metrics["popular_terms"]:
                    self.metrics["popular_terms"][word] = 0
                self.metrics["popular_terms"][word] += 1
        
        # Save updated metrics
        self._save_metrics()
    
    def _save_metrics(self) -> None:
        """Save metrics to file"""
        try:
            os.makedirs(os.path.dirname(self.metrics_file), exist_ok=True)
            with open(self.metrics_file, 'w', encoding='utf-8') as f:
                json.dump(self.metrics, f, indent=2)
        except Exception as e:
            print(f"Error saving metrics: {e}")
    
    def _sanitize_input(self, query: str) -> tuple:
        """
        Sanitize user input to prevent prompt injection
        Returns tuple of (sanitized_query, is_injection)
        """
        # Check for common prompt injection patterns
        injection_patterns = [
            r'ignore previous instructions',
            r'disregard (?:all|previous)',
            r'forget (?:all|your|previous)',
            r'new prompt:',
            r'system prompt:',
            r'new instructions:',
            r'you are now',
            r'you will be',
            r'your new role',
        ]
        
        for pattern in injection_patterns:
            if re.search(pattern, query, re.IGNORECASE):
                # Don't modify the query for metrics but flag as injection
                return (query, True)
        
        # Basic sanitization
        sanitized = re.sub(r'[^\w\s\.,\-\?:;\'\"()]', ' ', query)
        sanitized = re.sub(r'\s+', ' ', sanitized).strip()
        
        return (sanitized, False)
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get current metrics"""
        return self._load_metrics()
    
    def run(self, query: str) -> Dict[str, Any]:
        """Process an award-related query"""
        start_time = time.time()
        
        # Sanitize input
        sanitized_query, is_injection = self._sanitize_input(query)
        
        if is_injection:
            result = {
                "query": query,
                "response": "I can only provide information about Sundt Construction awards. Please rephrase your query.",
                "success": False,
                "reason": "Potential prompt injection detected"
            }
        else:
            # Search for relevant awards
            search_results = self.search_engine.search(sanitized_query, "awards", limit=5)
            awards = search_results.get("awards", [])
            
            if not awards:
                result = {
                    "query": sanitized_query,
                    "response": "I couldn't find any Sundt Construction awards matching your query. Would you like to try a different search term?",
                    "success": False,
                    "reason": "No matching awards found"
                }
            else:
                # Format award data for the prompt
                award_data = []
                for i, award in enumerate(awards, 1):
                    award_info = [f"AWARD {i}:"]
                    award_info.append(f"Title: {award.get('title', 'Untitled')}")
                    
                    for field in ["organization", "category", "award_type", "description", "location"]:
                        if field in award and award[field]:
                            award_info.append(f"{field.title()}: {award.get(field)}")
                    
                    # Add date/year if available
                    if "date" in award:
                        award_info.append(f"Date: {award.get('date')}")
                    elif "year" in award:
                        award_info.append(f"Year: {award.get('year')}")
                    
                    # Add project information if available
                    if "projects" in award and award["projects"]:
                        projects = award.get("projects")
                        if isinstance(projects, list):
                            project_titles = [p.get("title", "Unnamed Project") for p in projects]
                            award_info.append(f"Related Projects: {', '.join(project_titles)}")
                    
                    award_info.append("")
                    award_data.append("\n".join(award_info))
                
                # Generate response using LLM
                try:
                    response = self.chain.run(query=sanitized_query, award_data="\n".join(award_data))
                    
                    result = {
                        "query": sanitized_query,
                        "response": response,
                        "awards": awards,
                        "success": True
                    }
                except Exception as e:
                    result = {
                        "query": sanitized_query,
                        "response": "I encountered an error while processing your request about Sundt awards. Please try again.",
                        "success": False,
                        "reason": str(e)
                    }
        
        # Calculate execution time
        execution_time = time.time() - start_time
        result["execution_time"] = execution_time
        
        # Update metrics
        self._update_metrics(query, execution_time, is_injection)
        
        return result


# Example usage for testing
if __name__ == "__main__":
    # Create a .env file with OPENAI_API_KEY=your_api_key before running
    awards_agent = AwardsAgent()
    
    # Test queries
    test_queries = [
        "What safety awards has Sundt received?",
        "Tell me about Build America awards",
        "Has Sundt won any ENR awards in 2022?"
    ]
    
    for query in test_queries:
        print(f"\nQUERY: {query}")
        result = awards_agent.run(query)
        
        print(f"Success: {result['success']}")
        print(f"Execution time: {result['execution_time']:.2f} seconds")
        print("\nRESPONSE:")
        print(result["response"])
        print("\n" + "="*50)

================
File: projects_agent.py
================
import os
import json
import re
from datetime import datetime
import time
from typing import Dict, Any, List
from search_engine import SearchEngine
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from dotenv import load_dotenv

# Load environment variables from .env file (contains OPENAI_API_KEY)
load_dotenv()

class ProjectsAgent:
    """Agent specialized for retrieving project information"""
    
    def __init__(self, model_name="gpt-3.5-turbo", temperature=0.2):
        # Initialize search engine
        self.search_engine = SearchEngine()
        
        # Initialize OpenAI LLM
        self.model_name = model_name
        self.temperature = temperature
        self.llm = ChatOpenAI(model_name=model_name, temperature=temperature)
        
        # Set up the prompt template
        self.prompt_template = PromptTemplate(
            input_variables=["query", "project_data"],
            template="""
            You are the Projects Agent for Sundt Construction. Your role is to provide 
            accurate information about Sundt's past construction projects.
            
            USER QUERY: {query}
            
            PROJECT DATA:
            {project_data}
            
            Based on the information provided, respond to the user's query about Sundt's projects.
            Present the information in a clear, concise, and helpful manner.
            If the provided data doesn't contain relevant information to answer the query,
            say that you don't have that specific information about Sundt's projects.
            
            RESPONSE:
            """
        )
        
        # Create the chain
        self.chain = LLMChain(llm=self.llm, prompt=self.prompt_template)
        
        # Set up metrics tracking
        self.metrics_file = os.path.join("data", "projects_agent_metrics.json")
        self.metrics = self._load_metrics()
    
    def _load_metrics(self) -> Dict[str, Any]:
        """Load metrics from file or initialize if not exists"""
        if os.path.exists(self.metrics_file):
            try:
                with open(self.metrics_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return self._initialize_metrics()
        else:
            return self._initialize_metrics()
    
    def _initialize_metrics(self) -> Dict[str, Any]:
        """Initialize empty metrics structure"""
        return {
            "total_queries": 0,
            "query_times": [],
            "injection_attempts": [],
            "queries_by_date": {},
            "popular_terms": {}
        }
    
    def _update_metrics(self, query: str, execution_time: float, 
                       is_injection: bool = False) -> None:
        """Update metrics with query information"""
        # Load latest metrics
        self.metrics = self._load_metrics()
        
        # Get today's date as string
        today = datetime.now().strftime("%Y-%m-%d")
        
        # Update total queries
        self.metrics["total_queries"] += 1
        
        # Update query times (keep the last 100)
        self.metrics["query_times"].append(execution_time)
        if len(self.metrics["query_times"]) > 100:
            self.metrics["query_times"] = self.metrics["query_times"][-100:]
        
        # Log injection attempts
        if is_injection:
            self.metrics["injection_attempts"].append({
                "query": query,
                "date": today
            })
        
        # Update date-based metrics
        if today not in self.metrics["queries_by_date"]:
            self.metrics["queries_by_date"][today] = 0
        self.metrics["queries_by_date"][today] += 1
        
        # Update popular terms (simple word frequency)
        words = re.findall(r'\b\w+\b', query.lower())
        for word in words:
            if len(word) > 3:  # Only count words with more than 3 characters
                if word not in self.metrics["popular_terms"]:
                    self.metrics["popular_terms"][word] = 0
                self.metrics["popular_terms"][word] += 1
        
        # Save updated metrics
        self._save_metrics()
    
    def _save_metrics(self) -> None:
        """Save metrics to file"""
        try:
            os.makedirs(os.path.dirname(self.metrics_file), exist_ok=True)
            with open(self.metrics_file, 'w', encoding='utf-8') as f:
                json.dump(self.metrics, f, indent=2)
        except Exception as e:
            print(f"Error saving metrics: {e}")
    
    def _sanitize_input(self, query: str) -> tuple:
        """
        Sanitize user input to prevent prompt injection
        Returns tuple of (sanitized_query, is_injection)
        """
        # Check for common prompt injection patterns
        injection_patterns = [
            r'ignore previous instructions',
            r'disregard (?:all|previous)',
            r'forget (?:all|your|previous)',
            r'new prompt:',
            r'system prompt:',
            r'new instructions:',
            r'you are now',
            r'you will be',
            r'your new role',
        ]
        
        for pattern in injection_patterns:
            if re.search(pattern, query, re.IGNORECASE):
                # Don't modify the query for metrics but flag as injection
                return (query, True)
        
        # Basic sanitization
        sanitized = re.sub(r'[^\w\s\.,\-\?:;\'\"()]', ' ', query)
        sanitized = re.sub(r'\s+', ' ', sanitized).strip()
        
        return (sanitized, False)
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get current metrics"""
        return self._load_metrics()
    
    def run(self, query: str) -> Dict[str, Any]:
        """Process a project-related query"""
        start_time = time.time()
        
        # Sanitize input
        sanitized_query, is_injection = self._sanitize_input(query)
        
        if is_injection:
            result = {
                "query": query,
                "response": "I can only provide information about Sundt Construction projects. Please rephrase your query.",
                "success": False,
                "reason": "Potential prompt injection detected"
            }
        else:
            # Search for relevant projects
            search_results = self.search_engine.search(sanitized_query, "projects", limit=5)
            projects = search_results.get("projects", [])
            
            if not projects:
                result = {
                    "query": sanitized_query,
                    "response": "I couldn't find any Sundt Construction projects matching your query. Would you like to try a different search term?",
                    "success": False,
                    "reason": "No matching projects found"
                }
            else:
                # Format project data for the prompt
                project_data = []
                for i, project in enumerate(projects, 1):
                    project_info = [f"PROJECT {i}:"]
                    project_info.append(f"Title: {project.get('title', 'Untitled')}")
                    
                    if "description" in project:
                        project_info.append(f"Description: {project.get('description')}")
                    elif "overview" in project:
                        project_info.append(f"Overview: {project.get('overview')}")
                    
                    for field in ["location", "client", "value"]:
                        if field in project:
                            project_info.append(f"{field.title()}: {project.get(field)}")
                    
                    if "features" in project and project["features"]:
                        features = project.get("features")
                        features_text = ", ".join(features) if isinstance(features, list) else features
                        project_info.append(f"Features: {features_text}")
                    
                    project_info.append("")
                    project_data.append("\n".join(project_info))
                
                # Generate response using LLM
                try:
                    response = self.chain.run(query=sanitized_query, project_data="\n".join(project_data))
                    
                    result = {
                        "query": sanitized_query,
                        "response": response,
                        "projects": projects,
                        "success": True
                    }
                except Exception as e:
                    result = {
                        "query": sanitized_query,
                        "response": "I encountered an error while processing your request about Sundt projects. Please try again.",
                        "success": False,
                        "reason": str(e)
                    }
        
        # Calculate execution time
        execution_time = time.time() - start_time
        result["execution_time"] = execution_time
        
        # Update metrics
        self._update_metrics(query, execution_time, is_injection)
        
        return result


# Example usage for testing
if __name__ == "__main__":
    # Create a .env file with OPENAI_API_KEY=your_api_key before running
    projects_agent = ProjectsAgent()
    
    # Test queries
    test_queries = [
        "Tell me about water treatment projects",
        "What bridge projects has Sundt completed?",
        "Show me hospital construction projects in Arizona"
    ]
    
    for query in test_queries:
        print(f"\nQUERY: {query}")
        result = projects_agent.run(query)
        
        print(f"Success: {result['success']}")
        print(f"Execution time: {result['execution_time']:.2f} seconds")
        print("\nRESPONSE:")
        print(result["response"])
        print("\n" + "="*50)

================
File: requirements.txt
================
requests
beautifulsoup4
langchain
langchain-openai
openai
python-dotenv
fastapi
uvicorn
pydantic
tiktoken

================
File: search_engine.py
================
import os
import json
import re
from typing import List, Dict, Any, Union, Optional

class SearchEngine:
    """
    Simple search engine for Sundt data (projects and awards)
    Uses keyword-based search with basic relevance scoring
    """
    
    def __init__(self, data_dir="data"):
        self.data_dir = data_dir
        self.projects_file = os.path.join(data_dir, "projects.json")
        self.awards_file = os.path.join(data_dir, "awards.json")
        
        # Load data
        self.projects = self._load_json_data(self.projects_file, "projects")
        self.awards = self._load_json_data(self.awards_file, "awards")
        
        # Fields to search in for each data type
        self.project_search_fields = [
            "title", "description", "overview", "location", "client", 
            "specialties", "features", "impact.description", "testimonial"
        ]
        
        self.award_search_fields = [
            "title", "organization", "description", "location", 
            "category", "award_type", "projects.title"
        ]
    
    def _load_json_data(self, file_path: str, key: str) -> List[Dict[str, Any]]:
        """Load JSON data from file"""
        if not os.path.exists(file_path):
            print(f"Warning: {file_path} does not exist")
            return []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get(key, [])
        except Exception as e:
            print(f"Error loading data from {file_path}: {e}")
            return []
    
    def _get_nested_value(self, obj: Dict[str, Any], path: str) -> Optional[Union[str, List[str]]]:
        """
        Extract a value from a nested dictionary using dot notation
        For example: "impact.description" will get obj["impact"]["description"]
        Handles lists of objects too (e.g., projects.title)
        """
        if not path:
            return None
        
        parts = path.split('.')
        current = obj
        
        for i, part in enumerate(parts):
            if isinstance(current, dict) and part in current:
                current = current[part]
            elif isinstance(current, list):
                # If we're at the last part and it's a list of objects
                if i == len(parts) - 1:
                    values = []
                    for item in current:
                        if isinstance(item, dict) and part in item:
                            values.append(str(item[part]))
                    return " ".join(values) if values else None
                return None
            else:
                return None
        
        # Convert to string if it's a simple value, or join list items
        if isinstance(current, list):
            return " ".join(str(item) for item in current)
        return str(current) if current is not None else None
    
    def search_projects(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Search for projects matching the query"""
        if not query or not self.projects:
            return []
        
        # Tokenize query and create regex patterns
        query_terms = re.findall(r'\w+', query.lower())
        if not query_terms:
            return []
        
        # Score each project
        scored_projects = []
        for project in self.projects:
            score = 0
            matches = []
            
            for field in self.project_search_fields:
                value = self._get_nested_value(project, field)
                if not value:
                    continue
                
                # Score based on number of term matches
                value_lower = value.lower()
                field_matches = 0
                
                for term in query_terms:
                    # Count instances of the term
                    term_count = len(re.findall(r'\b' + re.escape(term) + r'\b', value_lower))
                    if term_count > 0:
                        field_matches += term_count
                        
                        # Record which field matched for explanation
                        matches.append(f"{field}: '{term}'")
                
                # Add to score, weigh certain fields more
                if field == "title":
                    score += field_matches * 3  # Title matches are more important
                elif field in ["description", "overview"]:
                    score += field_matches * 2  # Description matches are also important
                else:
                    score += field_matches
            
            if score > 0:
                scored_projects.append({
                    "project": project,
                    "score": score,
                    "matches": matches
                })
        
        # Sort by score (descending) and return projects
        scored_projects.sort(key=lambda x: x["score"], reverse=True)
        return [item["project"] for item in scored_projects[:limit]]
    
    def search_awards(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Search for awards matching the query"""
        if not query or not self.awards:
            return []
        
        # Tokenize query and create regex patterns
        query_terms = re.findall(r'\w+', query.lower())
        if not query_terms:
            return []
        
        # Score each award
        scored_awards = []
        for award in self.awards:
            score = 0
            matches = []
            
            for field in self.award_search_fields:
                value = self._get_nested_value(award, field)
                if not value:
                    continue
                
                # Score based on number of term matches
                value_lower = value.lower()
                field_matches = 0
                
                for term in query_terms:
                    # Count instances of the term
                    term_count = len(re.findall(r'\b' + re.escape(term) + r'\b', value_lower))
                    if term_count > 0:
                        field_matches += term_count
                        
                        # Record which field matched for explanation
                        matches.append(f"{field}: '{term}'")
                
                # Add to score, weigh certain fields more
                if field == "title":
                    score += field_matches * 3  # Title matches are more important
                elif field in ["organization", "category", "award_type"]:
                    score += field_matches * 2  # These matches are also important
                else:
                    score += field_matches
            
            if score > 0:
                scored_awards.append({
                    "award": award,
                    "score": score,
                    "matches": matches
                })
        
        # Sort by score (descending) and return awards
        scored_awards.sort(key=lambda x: x["score"], reverse=True)
        return [item["award"] for item in scored_awards[:limit]]
    
    def search(self, query: str, type: str = "all", limit: int = 10) -> Dict[str, Union[List[Dict[str, Any]], int]]:
        """
        Search for projects and/or awards matching the query
        
        Args:
            query: Search terms
            type: "projects", "awards", or "all"
            limit: Maximum number of results to return for each type
            
        Returns:
            Dictionary with results and timing info
        """
        results = {
            "query": query,
            "type": type,
        }
        
        if type == "projects" or type == "all":
            projects = self.search_projects(query, limit)
            results["projects"] = projects
            results["project_count"] = len(projects)
            
        if type == "awards" or type == "all":
            awards = self.search_awards(query, limit)
            results["awards"] = awards
            results["award_count"] = len(awards)
            
        return results


# Example usage
if __name__ == "__main__":
    engine = SearchEngine()
    
    # Example project search
    project_results = engine.search("water treatment", "projects", 5)
    print(f"Found {project_results['project_count']} matching projects")
    for i, project in enumerate(project_results.get("projects", [])):
        print(f"{i+1}. {project.get('title', 'Untitled')}")
        print(f"   {project.get('description', '')[:100]}...\n")
    
    # Example award search
    award_results = engine.search("safety excellence", "awards", 5)
    print(f"Found {award_results['award_count']} matching awards")
    for i, award in enumerate(award_results.get("awards", [])):
        print(f"{i+1}. {award.get('title', 'Untitled')}")
        print(f"   {award.get('organization', '')}")
        print(f"   {award.get('description', '')[:100]}...\n")

================
File: sundt_cli.py
================
import os
import dotenv
from projects_agent import ProjectsAgent
from awards_agent import AwardsAgent
import time

# Load environment variables
dotenv.load_dotenv()

class SundtCLI:
    def __init__(self):
        print("Initializing Sundt RAG CLI...")
        print("Loading agents...")
        self.projects_agent = ProjectsAgent()
        self.awards_agent = AwardsAgent()
        print("Agents loaded successfully!")
    
    def run(self):
        """Run the interactive CLI"""
        print("\n" + "="*60)
        print("               SUNDT CONSTRUCTION RAG SYSTEM")
        print("="*60)
        print("Welcome to the Sundt Construction RAG interface.")
        print("This system provides information about Sundt's projects and awards.")
        print("\nAvailable commands:")
        print("  projects <query> - Search for information about Sundt projects")
        print("  awards <query>   - Search for information about Sundt awards")
        print("  help             - Show this help message")
        print("  exit             - Exit the application")
        print("="*60)
        
        while True:
            try:
                user_input = input("\nEnter your command: ").strip()
                
                if not user_input:
                    continue
                    
                if user_input.lower() == "exit":
                    print("Thank you for using the Sundt RAG system. Goodbye!")
                    break
                    
                if user_input.lower() == "help":
                    print("\nAvailable commands:")
                    print("  projects <query> - Search for information about Sundt projects")
                    print("  awards <query>   - Search for information about Sundt awards")
                    print("  help             - Show this help message")
                    print("  exit             - Exit the application")
                    continue
                
                # Parse command and query
                parts = user_input.split(maxsplit=1)
                if len(parts) < 2:
                    print("Please provide a query after the command. Type 'help' for more information.")
                    continue
                    
                command, query = parts
                
                if command.lower() == "projects":
                    self._handle_projects_query(query)
                elif command.lower() == "awards":
                    self._handle_awards_query(query)
                else:
                    print(f"Unknown command: {command}")
                    print("Type 'help' to see available commands.")
            
            except KeyboardInterrupt:
                print("\nOperation cancelled by user.")
                break
            except Exception as e:
                print(f"Error: {str(e)}")
    
    def _handle_projects_query(self, query):
        """Process a query for the Projects agent"""
        print(f"\nSearching for projects related to: {query}")
        print("Processing...")
        
        start_time = time.time()
        result = self.projects_agent.run(query)
        duration = time.time() - start_time
        
        if result["success"]:
            print(f"\nFound {len(result.get('projects', []))} relevant projects")
            print(f"Response time: {duration:.2f} seconds")
            print("\nRESPONSE:")
            print(result["response"])
            
            # List the projects found
            if result.get("projects"):
                print("\nProjects found:")
                for i, project in enumerate(result["projects"], 1):
                    print(f"  {i}. {project.get('title', 'Untitled')}")
        else:
            print(f"\nNo results found. Reason: {result.get('reason', 'Unknown')}")
            print(f"Response: {result['response']}")
    
    def _handle_awards_query(self, query):
        """Process a query for the Awards agent"""
        print(f"\nSearching for awards related to: {query}")
        print("Processing...")
        
        start_time = time.time()
        result = self.awards_agent.run(query)
        duration = time.time() - start_time
        
        if result["success"]:
            print(f"\nFound {len(result.get('awards', []))} relevant awards")
            print(f"Response time: {duration:.2f} seconds")
            print("\nRESPONSE:")
            print(result["response"])
            
            # List the awards found
            if result.get("awards"):
                print("\nAwards found:")
                for i, award in enumerate(result["awards"], 1):
                    title = award.get('title', 'Untitled')
                    org = f" ({award.get('organization', '')})" if 'organization' in award else ""
                    print(f"  {i}. {title}{org}")
        else:
            print(f"\nNo results found. Reason: {result.get('reason', 'Unknown')}")
            print(f"Response: {result['response']}")

if __name__ == "__main__":
    # Check if OpenAI API key is set
    if not os.getenv("OPENAI_API_KEY"):
        print("ERROR: OPENAI_API_KEY environment variable not set.")
        print("Please create a .env file with your OpenAI API key:")
        print("OPENAI_API_KEY=your_api_key_here")
        exit(1)
    
    # Start the CLI
    cli = SundtCLI()
    cli.run()

================
File: test_search_engine.py
================
import sys
import json
from search_engine import SearchEngine

def test_search_engine():
    # Initialize the search engine
    engine = SearchEngine()
    
    # Print counts of loaded data
    print(f"Loaded {len(engine.projects)} projects")
    print(f"Loaded {len(engine.awards)} awards")
    
    # Test project searches
    print("\n--- PROJECT SEARCH TESTS ---")
    test_project_queries = [
        "water treatment",
        "transportation",
        "hospital",
        "bridge",
        "San Antonio"
    ]
    
    for query in test_project_queries:
        print(f"\nSearch query: '{query}'")
        results = engine.search_projects(query, limit=3)
        print(f"Found {len(results)} matching projects")
        
        for i, project in enumerate(results):
            print(f"  {i+1}. {project.get('title', 'Untitled')}")
            # Print a brief snippet of the description if available
            if "description" in project:
                desc = project["description"]
                print(f"     {desc[:100]}..." if len(desc) > 100 else f"     {desc}")
            elif "overview" in project:
                overview = project["overview"]
                print(f"     {overview[:100]}..." if len(overview) > 100 else f"     {overview}")
    
    # Test award searches
    print("\n--- AWARD SEARCH TESTS ---")
    test_award_queries = [
        "safety",
        "excellence",
        "build america",
        "ENR",
        "2022"
    ]
    
    for query in test_award_queries:
        print(f"\nSearch query: '{query}'")
        results = engine.search_awards(query, limit=3)
        print(f"Found {len(results)} matching awards")
        
        for i, award in enumerate(results):
            print(f"  {i+1}. {award.get('title', 'Untitled')}")
            if "organization" in award:
                print(f"     Organization: {award['organization']}")
            if "description" in award:
                desc = award["description"]
                print(f"     {desc[:100]}..." if len(desc) > 100 else f"     {desc}")
    
    # Test combined search
    print("\n--- COMBINED SEARCH TEST ---")
    combined_query = "bridge construction safety"
    print(f"Search query: '{combined_query}'")
    results = engine.search(combined_query, "all", limit=3)
    
    print(f"Found {results.get('project_count', 0)} matching projects")
    for i, project in enumerate(results.get('projects', [])):
        print(f"  Project {i+1}: {project.get('title', 'Untitled')}")
    
    print(f"Found {results.get('award_count', 0)} matching awards")
    for i, award in enumerate(results.get('awards', [])):
        print(f"  Award {i+1}: {award.get('title', 'Untitled')}")

if __name__ == "__main__":
    test_search_engine()
