This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2025-04-10T22:35:52.356Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
backend/
  crawlers/
    awards_crawler.py
    projects_crawler.py
  .gitignore
  api.py
  awards_agent.py
  projects_agent.py
  requirements.txt
  run_api.sh
  search_engine.py
  sundt_cli.py
  test_search_engine.py
frontend/
  .git/
    hooks/
      applypatch-msg.sample
      commit-msg.sample
      fsmonitor-watchman.sample
      post-update.sample
      pre-applypatch.sample
      pre-commit.sample
      pre-merge-commit.sample
      pre-push.sample
      pre-rebase.sample
      pre-receive.sample
      prepare-commit-msg.sample
      push-to-checkout.sample
      sendemail-validate.sample
      update.sample
    info/
      exclude
    refs/
      heads/
        master
    COMMIT_EDITMSG
    config
    description
    HEAD
  public/
    index.html
    robots.txt
  src/
    components/
      AwardsSearch.js
      ProjectsSearch.js
    App.css
    App.js
    App.test.js
    index.css
    index.js
    logo.svg
    reportWebVitals.js
    setupTests.js
  .gitignore
  README.md
.gitignore

================================================================
Repository Files
================================================================

================
File: backend/crawlers/awards_crawler.py
================
import os
import json
import requests
from bs4 import BeautifulSoup
import time
import re


class SundtAwardsCrawler:
    # Base URLs for the crawler
    BASE_URL = "https://www.sundt.com"
    AWARDS_URL = f"{BASE_URL}/about-us/awards-recognition/"

    def __init__(self, output_file="data/awards.json"):
        self.output_file = output_file
        # Create output directory if it doesn't exist
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        # Setup request headers to mimic a browser
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Content-Type": "application/json",
            "Accept": "application/json",
            "Referer": self.AWARDS_URL
        }

    def crawl(self):
        print(f"Crawling awards from {self.AWARDS_URL}")
        awards = []

        try:
            # Step 1: Get the main awards page
            response = requests.get(self.AWARDS_URL)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            
            # Step 2: Extract "New & Noteworthy" awards
            noteworthy_awards = self._extract_noteworthy_awards(soup)
            if noteworthy_awards:
                awards.extend(noteworthy_awards)
                print(f"Extracted {len(noteworthy_awards)} noteworthy awards")
            
            # Step 3: Extract "Additional Honors" awards (visible + hidden)
            additional_awards = self._extract_additional_honors(soup)
            if additional_awards:
                awards.extend(additional_awards)
                print(f"Extracted {len(additional_awards)} additional honors")
            
            # Step 4: Extract "News & Updates" awards
            news_awards = self._extract_news_updates(soup)
            if news_awards:
                awards.extend(news_awards)
                print(f"Extracted {len(news_awards)} news and updates")

        except Exception as e:
            print(f"Error crawling awards: {e}")

        # Save all data
        self._save_data(awards)
        print(f"Final count: {len(awards)} awards crawled")
        return awards

    def _extract_noteworthy_awards(self, soup):
        """Extract awards from the 'New & Noteworthy' section"""
        noteworthy_awards = []
        try:
            # Find the "New & Noteworthy" section
            noteworthy_section = soup.find("h4", class_="title-serif", string="New & Noteworthy")
            if not noteworthy_section:
                return noteworthy_awards
            
            # Find the container with the items
            items_container = noteworthy_section.find_parent("div", class_="section--cards")
            if not items_container:
                return noteworthy_awards
            
            # Find all award items
            award_items = items_container.select(".item")
            
            for item in award_items:
                # Extract title (header)
                title_elem = item.select_one(".item__head h3")
                title = title_elem.get_text(strip=True) if title_elem else ""
                
                # Extract description (body)
                desc_elem = item.select_one(".item__body p")
                description = desc_elem.get_text(strip=True) if desc_elem else ""
                
                # Extract icon/image if available
                img_elem = item.select_one(".item__image")
                image_url = ""
                if img_elem and "background-image" in img_elem.get("style", ""):
                    # Extract URL from background-image: url(...)
                    bg_img_match = re.search(r'url\((.*?)\)', img_elem.get("style", ""))
                    if bg_img_match:
                        image_url = bg_img_match.group(1).strip('\'"')
                        # Make relative URLs absolute
                        if image_url and not image_url.startswith(('http://', 'https://')):
                            image_url = self.BASE_URL + image_url if not image_url.startswith('/') else self.BASE_URL + image_url
                
                # Create award object
                award = {
                    "category": "New & Noteworthy",
                    "title": title,
                    "description": description,
                    "image_url": image_url
                }
                
                # Parse out award organization and date if possible
                if description:
                    # Try to extract organization and date from description
                    parts = description.split('/')
                    if len(parts) == 2:
                        award["organization"] = parts[0].strip()
                        award["date"] = parts[1].strip()
                    # If only one part, check if it's a date
                    elif len(parts) == 1 and re.search(r'\b(January|February|March|April|May|June|July|August|September|October|November|December)\b', description):
                        award["date"] = description.strip()
                
                noteworthy_awards.append(award)
                
        except Exception as e:
            print(f"Error extracting noteworthy awards: {e}")
            
        return noteworthy_awards

    def _extract_additional_honors(self, soup):
        """Extract awards from the 'Additional Honors' section, including hidden awards"""
        additional_awards = []
        try:
            # Find the "Additional Honors" section
            honors_section = soup.find("h5", class_="section__sub-title", string="Additional Honors")
            if not honors_section:
                return additional_awards
            
            # Find the container with the columns
            columns_container = honors_section.find_parent("div", class_="awardsSection")
            if not columns_container:
                return additional_awards
            
            # Process visible and hidden awards from both columns
            for column in columns_container.select(".col"):
                # Process visible awards in this column
                visible_awards = self._parse_awards_from_column(column, exclude_hidden=True)
                additional_awards.extend(visible_awards)
                
                # Process hidden awards in this column (those in div with class "hidden")
                hidden_div = column.select_one(".hidden")
                if hidden_div:
                    hidden_awards = self._parse_awards_from_column(hidden_div, exclude_hidden=False)
                    additional_awards.extend(hidden_awards)
                
        except Exception as e:
            print(f"Error extracting additional honors: {e}")
            
        return additional_awards
    
    def _parse_awards_from_column(self, column, exclude_hidden=False):
        """Parse award entries from a column in the Additional Honors section"""
        awards = []
        
        # Skip processing if this is a hidden div and we want to exclude hidden content
        if exclude_hidden and "hidden" in column.get("class", []):
            return awards
        
        # Group content by award entries
        # Each award typically has an h6 header followed by p tags
        current_award = None
        
        for elem in column.find_all(["h6", "p"]):
            # Skip if this element is within a hidden div and we want to exclude hidden
            if exclude_hidden and elem.find_parent(class_="hidden"):
                continue
                
            if elem.name == "h6":
                # Save previous award if exists
                if current_award:
                    awards.append(current_award)
                
                # Start a new award
                current_award = {
                    "category": "Additional Honors",
                    "award_type": elem.get_text(strip=True).replace("<em>", "").replace("</em>", "")
                }
            elif elem.name == "p" and current_award:
                # Add content to the current award
                content = elem.get_text(strip=True)
                
                # First p tag after h6 is usually the organization
                if "organization" not in current_award:
                    strong_elem = elem.find("strong")
                    if strong_elem:
                        current_award["organization"] = strong_elem.get_text(strip=True)
                        
                        # Remove organization from content for description
                        content = content.replace(current_award["organization"], "").strip()
                    
                # Check if there's a project link
                link_elem = elem.find("a")
                if link_elem:
                    project_url = link_elem.get("href")
                    project_title = link_elem.get_text(strip=True)
                    
                    if "projects" not in current_award:
                        current_award["projects"] = []
                    
                    current_award["projects"].append({
                        "title": project_title,
                        "url": project_url if project_url.startswith(('http://', 'https://')) else self.BASE_URL + project_url if not project_url.startswith('/') else self.BASE_URL + project_url
                    })
                
                # Add remaining content as description
                if content and content != current_award.get("organization", ""):
                    if "description" not in current_award:
                        current_award["description"] = content
                    else:
                        current_award["description"] += " " + content
        
        # Don't forget to add the last award
        if current_award:
            awards.append(current_award)
            
        # Process descriptions to extract location and year
        for award in awards:
            if "description" in award:
                # Try to extract location (usually at the end before the year)
                location_match = re.search(r'([A-Za-z\s]+),\s*([A-Z]{2})\s*\d{4}', award["description"])
                if location_match:
                    award["location"] = f"{location_match.group(1)}, {location_match.group(2)}".strip()
                
                # Try to extract year (usually a 4-digit number at the end)
                year_match = re.search(r'\b(20\d{2})\b', award["description"])
                if year_match:
                    award["year"] = year_match.group(1)
        
        return awards

    def _extract_news_updates(self, soup):
        """Extract awards from the 'News & Updates' section"""
        news_awards = []
        try:
            # Find the "News & Updates" section
            news_section = soup.find("h5", class_="section__sub-title", string="News & Updates")
            if not news_section:
                return news_awards
            
            # Find the container with the slider
            slider_container = news_section.find_parent("div", class_="section--card-slider")
            if not slider_container:
                return news_awards
            
            # Find all slides
            slides = slider_container.select(".slider__slide")
            
            for slide in slides:
                # Extract card content
                card = slide.select_one(".card")
                if not card:
                    continue
                
                # Extract link
                link_elem = card.select_one("a")
                link = link_elem.get("href") if link_elem else ""
                if link and not link.startswith(('http://', 'https://')):
                    link = self.BASE_URL + link if not link.startswith('/') else self.BASE_URL + link
                
                # Extract title
                title_elem = card.select_one("h4")
                title = title_elem.get_text(strip=True) if title_elem else ""
                
                # Extract description
                desc_elem = card.select_one(".card-body")
                description = ""
                if desc_elem:
                    # Get text excluding the title
                    desc_text = desc_elem.get_text(strip=True)
                    if title in desc_text:
                        description = desc_text.replace(title, "").strip()
                    else:
                        description = desc_text
                
                # Extract image
                img_elem = card.select_one(".card-image")
                image_url = ""
                if img_elem and "background-image" in img_elem.get("style", ""):
                    # Extract URL from background-image: url(...)
                    bg_img_match = re.search(r'url\((.*?)\)', img_elem.get("style", ""))
                    if bg_img_match:
                        image_url = bg_img_match.group(1).strip('\'"')
                        # Make relative URLs absolute
                        if image_url and not image_url.startswith(('http://', 'https://')):
                            image_url = self.BASE_URL + image_url if not image_url.startswith('/') else self.BASE_URL + image_url
                
                # Create news award object
                news_award = {
                    "category": "News & Updates",
                    "title": title,
                    "description": description,
                    "url": link,
                    "image_url": image_url
                }
                
                news_awards.append(news_award)
                
        except Exception as e:
            print(f"Error extracting news updates: {e}")
            
        return news_awards

    def _save_data(self, awards):
        """Save the extracted award data to a JSON file."""
        try:
            # Process the awards to create a well-structured output
            processed_awards = process_awards(awards)
            with open(self.output_file, "w", encoding="utf-8") as f:
                json.dump({"awards": processed_awards}, f, indent=2)
            print(f"Saved {len(processed_awards)} awards to {self.output_file}")
        except Exception as e:
            print(f"Error saving data: {e}")

def process_awards(awards):
    """Process the crawled awards to create a well-structured final output."""
    processed_awards = []

    for award in awards:
        # Start with a clean award object
        processed_award = {
            "category": award.get("category", ""),
            "title": award.get("title", award.get("award_type", ""))
        }
        
        # Add organization if available
        if "organization" in award:
            processed_award["organization"] = award["organization"]
            
        # Add description if available
        if "description" in award:
            processed_award["description"] = award["description"]
            
        # Add location if available
        if "location" in award:
            processed_award["location"] = award["location"]
            
        # Add date/year if available
        if "date" in award:
            processed_award["date"] = award["date"]
        elif "year" in award:
            processed_award["year"] = award["year"]
            
        # Add image if available
        if "image_url" in award and award["image_url"]:
            processed_award["image_url"] = award["image_url"]
            
        # Add URL if available
        if "url" in award and award["url"]:
            processed_award["url"] = award["url"]
            
        # Add projects if available
        if "projects" in award and award["projects"]:
            processed_award["projects"] = award["projects"]
        
        processed_awards.append(processed_award)

    return processed_awards

if __name__ == "__main__":
    crawler = SundtAwardsCrawler()
    awards = crawler.crawl()
    print(f"Extracted {len(awards)} awards in total")

================
File: backend/crawlers/projects_crawler.py
================
import os
import json
import requests
from bs4 import BeautifulSoup
import time
import re

class SundtProjectsCrawler:
    # Base URLs for the crawler
    BASE_URL = "https://www.sundt.com"
    PROJECTS_URL = f"{BASE_URL}/projects/"
    AJAX_URL = f"{BASE_URL}/wp-json/facetwp/v1/refresh"  # FacetWP AJAX endpoint

    def __init__(self, output_file="data/projects.json"):
        self.output_file = output_file
        # Create output directory if it doesn't exist
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        # Setup request headers to mimic a browser
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Content-Type": "application/json",
            "Accept": "application/json",
            "Referer": self.PROJECTS_URL  # Important for AJAX requests
        }
        # Track project URLs we've already seen to avoid duplicates
        self.seen_project_urls = set()

    def crawl(self):
        print(f"Crawling projects from {self.PROJECTS_URL}")
        projects = []

        try:
            # Step 1: Get the main projects page
            response = requests.get(self.PROJECTS_URL)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            
            # Extract projects from the first page
            project_cards = soup.select(".project-card")
            print(f"Processing {len(project_cards)} projects from first page")

            for card in project_cards:
                project_data = self._extract_project_data(card)
                if project_data and project_data['url'] not in self.seen_project_urls:
                    self.seen_project_urls.add(project_data['url'])
                    projects.append(project_data)

            # Step 2: Extract FacetWP settings for pagination
            fwp_settings = self._extract_fwp_settings(response.text)

            # Step 3: Handle pagination based on whether we found FacetWP settings
            if not fwp_settings:
                # Fallback: Try direct pagination if FacetWP settings couldn't be extracted
                print("Could not extract FacetWP settings. Will try direct pagination approach.")
                page = 2
                max_attempts = 20  # Safety limit to prevent infinite loops

                while page <= max_attempts:
                    print(f"Trying direct page {page}...")
                    page_url = f"{self.PROJECTS_URL}page/{page}/"
                    try:
                        page_response = requests.get(page_url)
                        if page_response.status_code != 200:
                            print(f"Reached end of pagination at page {page}")
                            break

                        page_soup = BeautifulSoup(page_response.text, "html.parser")
                        page_cards = page_soup.select(".project-card")
                        if not page_cards:
                            print(f"No project cards found on page {page}")
                            break

                        print(f"Processing {len(page_cards)} projects from page {page}")
                        for card in page_cards:
                            project_data = self._extract_project_data(card)
                            if project_data and project_data['url'] not in self.seen_project_urls:
                                self.seen_project_urls.add(project_data['url'])
                                projects.append(project_data)

                        page += 1
                        time.sleep(1)  # Be nice to the server
                    except Exception as e:
                        print(f"Error loading page {page}: {e}")
                        break
            else:
                # Use AJAX pagination with FacetWP settings
                total_pages = fwp_settings.get('total_pages', 20)  # Default to 20 if not found
                print(f"Found FacetWP settings. Total pages estimated: {total_pages}")

                # Process remaining pages using AJAX
                for page in range(2, total_pages + 1):
                    print(f"Loading page {page} of {total_pages} via AJAX")
                    
                    # Create payload with correct FacetWP structure
                    payload = {
                        "action": "facetwp_refresh",
                        "data": {
                            "extras": {"selections": True, "sort": "default"},
                            "facets": {
                                "region": [], "project_delivery_methods": [],
                                "project_markets": [], "project_submarket": [], "count": []
                            },
                            "first_load": 0,
                            "frozen_facets": {},
                            "http_params": {"get": [], "uri": "projects", "url_vars": []},
                            "is_bfcache": 1,
                            "paged": page,  # Current page number
                            "soft_refresh": 1,
                            "template": "projects"
                        }
                    }

                    try:
                        # Make AJAX request to load more projects
                        ajax_response = requests.post(
                            self.AJAX_URL,
                            headers=self.headers,
                            json=payload
                        )

                        if ajax_response.status_code != 200:
                            print(f"Error loading page {page}: Status {ajax_response.status_code}")
                            print(f"Response: {ajax_response.text[:200]}...")  # Print start of response
                            continue

                        try:
                            # Parse AJAX response
                            ajax_data = ajax_response.json()
                            
                            # Check different possible response structures
                            html_content = (
                                ajax_data.get('template') or
                                ajax_data.get('html') or
                                ajax_data.get('content')
                            )

                            if not html_content:
                                print(f"No HTML content returned for page {page}")
                                print(f"Response keys: {ajax_data.keys()}")
                                continue

                            # Extract projects from AJAX response
                            ajax_soup = BeautifulSoup(html_content, "html.parser")
                            ajax_project_cards = ajax_soup.select(".project-card")
                            if not ajax_project_cards:
                                print(f"No project cards found in AJAX response for page {page}")
                                continue

                            print(f"Processing {len(ajax_project_cards)} projects from page {page}")
                            for card in ajax_project_cards:
                                project_data = self._extract_project_data(card)
                                if project_data and project_data['url'] not in self.seen_project_urls:
                                    self.seen_project_urls.add(project_data['url'])
                                    projects.append(project_data)
                        except json.JSONDecodeError as e:
                            print(f"Error decoding JSON from AJAX response: {e}")
                            print(f"Response text: {ajax_response.text[:100]}...")
                    except Exception as e:
                        print(f"Error making AJAX request: {e}")

                    time.sleep(1)  # Be nice to the server

            # Step 4: Get detailed information for each project
            print(f"Getting detailed information for {len(projects)} projects...")
            for i, project in enumerate(projects):
                if 'url' in project and project['url']:
                    print(f"Processing project {i+1}/{len(projects)}: {project['title']}")
                    detailed_data = self._get_project_details(project['url'])
                    if detailed_data:
                        project.update(detailed_data)

                # Save intermediate results every 10 projects
                if (i + 1) % 10 == 0:
                    print(f"Processed {i + 1}/{len(projects)} project details")
                    self._save_data(projects)  # Checkpoint save

                time.sleep(0.5)  # Be nice to the server

        except Exception as e:
            print(f"Error crawling projects: {e}")

        # Final save of all data
        self._save_data(projects)
        print(f"Final count: {len(projects)} unique projects crawled")
        return projects

    def _extract_fwp_settings(self, html_content):
        """Extract FacetWP pagination settings from the HTML."""
        try:
            # Try multiple regex patterns to find FacetWP settings
            patterns = [
                r"window\.FWP_JSON\s*=\s*(\{.*?\});.*?window\.FWP_HTTP",
                r"var\s+FWP_JSON\s*=\s*(\{.*?\});",
                r"FWP\.preload_data\s*=\s*(\{.*?\});"
            ]

            for pattern in patterns:
                matches = re.search(pattern, html_content, re.DOTALL)
                if matches:
                    json_str = matches.group(1)
                    try:
                        data = json.loads(json_str)
                        # Try different JSON structures to find pagination info
                        if 'preload_data' in data and 'settings' in data['preload_data']:
                            return data['preload_data']['settings'].get('pager')
                        elif 'settings' in data and 'pager' in data['settings']:
                            return data['settings']['pager']
                        return data
                    except json.JSONDecodeError:
                        continue

            # Fallback: Look for total_pages directly in the HTML
            total_pages_match = re.search(r'total_pages["\']?\s*:\s*(\d+)', html_content)
            if total_pages_match:
                return {"total_pages": int(total_pages_match.group(1))}

            # Fallback: Look for pagination elements in the HTML
            soup = BeautifulSoup(html_content, 'html.parser')
            pagination = soup.select('.pagination, .nav-links, .facetwp-pager')
            if pagination:
                page_links = pagination[0].select('a')
                highest_page = max([
                    int(link.get_text(strip=True))
                    for link in page_links
                    if link.get_text(strip=True).isdigit()
                ], default=1)
                return {"total_pages": highest_page}
        except Exception as e:
            print(f"Error extracting FacetWP settings: {e}")

        return None

    def _extract_project_data(self, card):
        """Extract basic project data from a project card element."""
        try:
            # Find the link element (multiple possible selectors)
            link_element = card.select_one("a.project-card__link, a.card-link, .project-card a, .project a")
            if not link_element:
                return None

            # Get the project URL and ensure it's absolute
            project_url = link_element.get("href")
            if project_url and not project_url.startswith(('http://', 'https://')):
                project_url = self.BASE_URL + project_url if not project_url.startswith('/') else self.BASE_URL + project_url

            # Get the project title
            title_element = card.select_one(".project-card__title, .card-title, h2, h3, .title")
            title_text = title_element.get_text(strip=True) if title_element else "Unknown Project"
            title = re.sub(r'▶|→|⇒', '', title_text).strip()  # Remove arrow symbols

            # Get the project image
            img_element = card.select_one(".project-card__image img, .card-img img, img")
            image_url = img_element.get("src") if img_element else ""
            if image_url and not image_url.startswith(('http://', 'https://')):
                image_url = self.BASE_URL + image_url if not image_url.startswith('/') else self.BASE_URL + image_url

            # Get any snippet/excerpt text
            snippet_element = card.select_one(".project-card__excerpt, .card-text, .excerpt, p")
            snippet = snippet_element.get_text(strip=True) if snippet_element else ""

            return {
                "title": title,
                "url": project_url,
                "image_url": image_url,
                "snippet": snippet
            }
        except Exception as e:
            print(f"Error extracting project data from card: {e}")
            return None

    def _get_project_details(self, project_url):
        """Get detailed project information from the project page."""
        try:
            full_url = project_url if project_url.startswith("http") else self.BASE_URL + project_url
            response = requests.get(full_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            # Initialize the detailed data structure
            detailed_data = {
                "metadata": {},  # Will hold location, client, value, etc.
                "impact": {},    # Will hold Community Impact section
                "overview": "",  # Will hold Project Overview text
                "features": []   # Will hold Features & Highlights
            }

            # Extract metadata from the list-info section
            metadata_items = soup.select(".list-info li")
            for item in metadata_items:
                label_elem = item.select_one("h5")
                value_elem = item.select_one("p")
                if label_elem and value_elem:
                    # Clean up the label for use as a key
                    label = label_elem.get_text(strip=True).rstrip(':').lower().replace(' ', '_')
                    value = value_elem.get_text(strip=True)
                    
                    # Handle specialties as a list (comma-separated)
                    if label == "specialties":
                        detailed_data["metadata"][label] = [s.strip() for s in value.split(',')]
                    else:
                        detailed_data["metadata"][label] = value

            # Extract the Community Impact section
            impact_section = soup.find("h5", string=lambda text: text and "Community Impact" in text)
            if impact_section:
                impact_container = impact_section.find_parent("div", class_="ModalContainer")
                if impact_container:
                    # Get the impact title
                    impact_title = impact_container.select_one("h3")
                    if impact_title:
                        detailed_data["impact"]["title"] = impact_title.get_text(strip=True)
                    
                    # Get the impact description
                    impact_desc = impact_container.select_one("p")
                    if impact_desc:
                        detailed_data["impact"]["description"] = impact_desc.get_text(strip=True)

            # Extract Project Overview
            overview_section = soup.find("h6", string=lambda text: text and "Project Overview" in text)
            if overview_section:
                overview_container = overview_section.find_parent("div", class_="section__content")
                if overview_container:
                    # Get all paragraphs in the overview section
                    overview_paragraphs = overview_container.select("p:not([id])")
                    overview_text = " ".join([p.get_text(strip=True) for p in overview_paragraphs])
                    
                    # Also get any collapsed content (Read More sections)
                    collapsed_content = overview_container.select(".content.collapse p")
                    if collapsed_content:
                        overview_text += " " + " ".join([p.get_text(strip=True) for p in collapsed_content])
                    
                    detailed_data["overview"] = overview_text

            # Extract Features & Highlights
            features_section = soup.find("h3", string=lambda text: text and "Features & Highlights" in text)
            if features_section:
                features_container = features_section.find_parent("div", class_="section__aside")
                if features_container:
                    # Get all bullet points
                    feature_items = features_container.select("ul.list-bullets li")
                    detailed_data["features"] = [item.get_text(strip=True) for item in feature_items]

            # Extract any testimonial/quote
            blockquote = soup.select_one("blockquote")
            if blockquote:
                quote_text = blockquote.get_text(strip=True)
                if quote_text:
                    detailed_data["testimonial"] = quote_text

            return detailed_data
        except Exception as e:
            print(f"Error getting project details from {project_url}: {e}")
            return {}

    def _save_data(self, projects):
        """Save the extracted project data to a JSON file."""
        try:
            # Process the projects to create a well-structured output
            processed_projects = process_projects(projects)
            with open(self.output_file, "w", encoding="utf-8") as f:
                json.dump({"projects": processed_projects}, f, indent=2)
            print(f"Saved {len(processed_projects)} projects to {self.output_file}")
        except Exception as e:
            print(f"Error saving data: {e}")

def process_projects(projects):
    """Process the crawled projects to create a well-structured final output."""
    processed_projects = []

    for project in projects:
        # Start with basic project info
        processed_project = {
            "title": project.get("title", ""),
            "url": project.get("url", ""),
            "image_url": project.get("image_url", "")
        }

        # Add metadata as top-level fields
        if "metadata" in project:
            for key, value in project["metadata"].items():
                processed_project[key] = value

        # Add overview as description
        if "overview" in project and project["overview"]:
            processed_project["description"] = project["overview"]

        # Add impact section if available
        if "impact" in project and project["impact"]:
            processed_project["impact"] = project["impact"]

        # Add features if available
        if "features" in project and project["features"]:
            processed_project["features"] = project["features"]

        # Add testimonial if available
        if "testimonial" in project:
            processed_project["testimonial"] = project["testimonial"]

        processed_projects.append(processed_project)

    return processed_projects

if __name__ == "__main__":
    crawler = SundtProjectsCrawler()
    projects = crawler.crawl()
    print(f"Extracted {len(projects)} projects in total")

================
File: backend/.gitignore
================
###############################################################################
#  General Python
###############################################################################
# Byte‑compiled / optimized files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

###############################################################################
#  Virtual Environments
###############################################################################
# (adjust names if you use a different folder)
venv/
env/
ENV/
.venv/
.conda/

###############################################################################
#  Packaging / Build artifacts
###############################################################################
build/
dist/
*.egg-info/
.eggs/
*.egg
pip-wheel-metadata/
*.whl

###############################################################################
#  Testing / Coverage
###############################################################################
.pytest_cache/
.nox/
.tox/
.coverage
coverage.xml
nosetests.xml
htmlcov/
.hypothesis/

###############################################################################
#  Jupyter
###############################################################################
.ipynb_checkpoints/

###############################################################################
#  IDE / Editor folders
###############################################################################
.vscode/
.idea/

###############################################################################
#  OS‑specific
###############################################################################
.DS_Store
Thumbs.db

###############################################################################
#  Project‑specific exclusions
###############################################################################
# Environment variables
.env

# Raw or generated data you don’t want in Git
data/

================
File: backend/api.py
================
import os
from typing import Optional, Dict, Any, List
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

# Import our existing agents and search engine
from projects_agent import ProjectsAgent
from awards_agent import AwardsAgent
from search_engine import SearchEngine

# Load environment variables
load_dotenv()

# Initialize FastAPI app
app = FastAPI(
    title="Sundt RAG API",
    description="API for querying Sundt Construction projects and awards",
    version="1.0.0"
)

# Add CORS middleware to allow frontend to call the API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # For development; restrict this in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize our agents and search engine
projects_agent = ProjectsAgent()
awards_agent = AwardsAgent()
search_engine = SearchEngine()

# Check if data files exist and log warning if not
if not os.path.exists("data/projects.json"):
    print("WARNING: Projects data file not found. Run the crawler first.")

if not os.path.exists("data/awards.json"):
    print("WARNING: Awards data file not found. Run the crawler first.")

# Define request and response models
class SearchRequest(BaseModel):
    query: str

class ProjectResponse(BaseModel):
    title: str
    url: Optional[str] = None
    image_url: Optional[str] = None
    description: Optional[str] = None
    location: Optional[str] = None
    client: Optional[str] = None
    value: Optional[str] = None
    features: Optional[List[str]] = None

class AwardResponse(BaseModel):
    title: str
    organization: Optional[str] = None
    category: Optional[str] = None
    description: Optional[str] = None
    date: Optional[str] = None
    year: Optional[str] = None
    image_url: Optional[str] = None

class ProjectsSearchResponse(BaseModel):
    query: str
    response: str
    projects: List[Dict[str, Any]]
    execution_time: float
    success: bool

class AwardsSearchResponse(BaseModel):
    query: str
    response: str
    awards: List[Dict[str, Any]]
    execution_time: float
    success: bool

class BasicSearchResponse(BaseModel):
    query: str
    type: str
    projects: Optional[List[Dict[str, Any]]] = None
    awards: Optional[List[Dict[str, Any]]] = None
    project_count: Optional[int] = None
    award_count: Optional[int] = None

class MetricsResponse(BaseModel):
    projects: Dict[str, Any]
    awards: Dict[str, Any]

# API Endpoints
@app.get("/", tags=["General"])
async def root():
    """Root endpoint for API verification"""
    return {
        "message": "Sundt RAG API is running",
        "status": "online",
        "endpoints": {
            "search": "/search?query={query}&type={type}",
            "projects": "/projects?query={query}",
            "awards": "/awards?query={query}",
            "metrics": "/metrics"
        }
    }

@app.get("/search", response_model=BasicSearchResponse, tags=["Search"])
async def search(
    query: str = Query(..., description="Search query"),
    type: str = Query("all", description="Search type: 'projects', 'awards', or 'all'")
):
    """
    Basic search for both projects and awards using the search engine directly
    """
    if not query.strip():
        raise HTTPException(status_code=400, detail="Query cannot be empty")
    
    if type not in ["all", "projects", "awards"]:
        raise HTTPException(status_code=400, detail="Type must be 'all', 'projects', or 'awards'")
    
    # Perform the search
    results = search_engine.search(query, type)
    return results

@app.post("/projects", response_model=ProjectsSearchResponse, tags=["Projects"])
async def query_projects(request: SearchRequest):
    """
    Query the Projects Agent for intelligent responses about Sundt projects
    """
    if not request.query.strip():
        raise HTTPException(status_code=400, detail="Query cannot be empty")
    
    # Process the query through the Projects Agent
    result = projects_agent.run(request.query)
    
    if not result["success"] and "injection" in result.get("reason", "").lower():
        raise HTTPException(status_code=400, detail="Potential prompt injection detected")
    
    return result

@app.post("/awards", response_model=AwardsSearchResponse, tags=["Awards"])
async def query_awards(request: SearchRequest):
    """
    Query the Awards Agent for intelligent responses about Sundt awards
    """
    if not request.query.strip():
        raise HTTPException(status_code=400, detail="Query cannot be empty")
    
    # Process the query through the Awards Agent
    result = awards_agent.run(request.query)
    
    if not result["success"] and "injection" in result.get("reason", "").lower():
        raise HTTPException(status_code=400, detail="Potential prompt injection detected")
    
    return result

@app.get("/metrics", response_model=MetricsResponse, tags=["Admin"])
async def get_metrics():
    """
    Get usage metrics for projects and awards agents
    """
    projects_metrics = projects_agent.get_metrics()
    awards_metrics = awards_agent.get_metrics()
    
    return {
        "projects": projects_metrics,
        "awards": awards_metrics
    }

# Additional endpoints for completeness

@app.get("/projects/list", response_model=List[ProjectResponse], tags=["Projects"])
async def list_projects(limit: int = Query(10, description="Maximum number of projects to return")):
    """
    List all projects with optional limit
    """
    # Limit the number of projects to return
    projects = search_engine.projects[:limit]
    return projects

@app.get("/awards/list", response_model=List[AwardResponse], tags=["Awards"])
async def list_awards(limit: int = Query(10, description="Maximum number of awards to return")):
    """
    List all awards with optional limit
    """
    # Limit the number of awards to return
    awards = search_engine.awards[:limit]
    return awards

if __name__ == "__main__":
    import uvicorn
    # Run the FastAPI app with uvicorn
    uvicorn.run("api:app", host="0.0.0.0", port=8000, reload=True)

================
File: backend/awards_agent.py
================
import os
import json
import re
from datetime import datetime
import time
from typing import Dict, Any, List
from search_engine import SearchEngine
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from dotenv import load_dotenv

# Load environment variables from .env file (contains OPENAI_API_KEY)
load_dotenv()

class AwardsAgent:
    """Agent specialized for retrieving award information"""
    
    def __init__(self, model_name="gpt-3.5-turbo", temperature=0.2):
        # Initialize search engine
        self.search_engine = SearchEngine()
        
        # Initialize OpenAI LLM
        self.model_name = model_name
        self.temperature = temperature
        self.llm = ChatOpenAI(model_name=model_name, temperature=temperature)
        
        # Set up the prompt template
        self.prompt_template = PromptTemplate(
            input_variables=["query", "award_data"],
            template="""
            You are the Awards Agent for Sundt Construction. Your role is to provide 
            accurate information about awards and recognition received by Sundt.
            
            USER QUERY: {query}
            
            AWARD DATA:
            {award_data}
            
            Based on the information provided, respond to the user's query about Sundt's awards.
            Present the information in a clear, concise, and helpful manner.
            If the provided data doesn't contain relevant information to answer the query,
            say that you don't have that specific information about Sundt's awards.
            
            RESPONSE:
            """
        )
        
        # Create the chain
        self.chain = LLMChain(llm=self.llm, prompt=self.prompt_template)
        
        # Set up metrics tracking
        self.metrics_file = os.path.join("data", "awards_agent_metrics.json")
        self.metrics = self._load_metrics()
    
    def _load_metrics(self) -> Dict[str, Any]:
        """Load metrics from file or initialize if not exists"""
        if os.path.exists(self.metrics_file):
            try:
                with open(self.metrics_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return self._initialize_metrics()
        else:
            return self._initialize_metrics()
    
    def _initialize_metrics(self) -> Dict[str, Any]:
        """Initialize empty metrics structure"""
        return {
            "total_queries": 0,
            "query_times": [],
            "injection_attempts": [],
            "queries_by_date": {},
            "popular_terms": {}
        }
    
    def _update_metrics(self, query: str, execution_time: float, 
                       is_injection: bool = False) -> None:
        """Update metrics with query information"""
        # Load latest metrics
        self.metrics = self._load_metrics()
        
        # Get today's date as string
        today = datetime.now().strftime("%Y-%m-%d")
        
        # Update total queries
        self.metrics["total_queries"] += 1
        
        # Update query times (keep the last 100)
        self.metrics["query_times"].append(execution_time)
        if len(self.metrics["query_times"]) > 100:
            self.metrics["query_times"] = self.metrics["query_times"][-100:]
        
        # Log injection attempts
        if is_injection:
            self.metrics["injection_attempts"].append({
                "query": query,
                "date": today
            })
        
        # Update date-based metrics
        if today not in self.metrics["queries_by_date"]:
            self.metrics["queries_by_date"][today] = 0
        self.metrics["queries_by_date"][today] += 1
        
        # Update popular terms (simple word frequency)
        words = re.findall(r'\b\w+\b', query.lower())
        for word in words:
            if len(word) > 3:  # Only count words with more than 3 characters
                if word not in self.metrics["popular_terms"]:
                    self.metrics["popular_terms"][word] = 0
                self.metrics["popular_terms"][word] += 1
        
        # Save updated metrics
        self._save_metrics()
    
    def _save_metrics(self) -> None:
        """Save metrics to file"""
        try:
            os.makedirs(os.path.dirname(self.metrics_file), exist_ok=True)
            with open(self.metrics_file, 'w', encoding='utf-8') as f:
                json.dump(self.metrics, f, indent=2)
        except Exception as e:
            print(f"Error saving metrics: {e}")
    
    def _sanitize_input(self, query: str) -> tuple:
        """
        Sanitize user input to prevent prompt injection
        Returns tuple of (sanitized_query, is_injection)
        """
        # Check for common prompt injection patterns
        injection_patterns = [
            r'ignore previous instructions',
            r'disregard (?:all|previous)',
            r'forget (?:all|your|previous)',
            r'new prompt:',
            r'system prompt:',
            r'new instructions:',
            r'you are now',
            r'you will be',
            r'your new role',
        ]
        
        for pattern in injection_patterns:
            if re.search(pattern, query, re.IGNORECASE):
                # Don't modify the query for metrics but flag as injection
                return (query, True)
        
        # Basic sanitization
        sanitized = re.sub(r'[^\w\s\.,\-\?:;\'\"()]', ' ', query)
        sanitized = re.sub(r'\s+', ' ', sanitized).strip()
        
        return (sanitized, False)
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get current metrics"""
        return self._load_metrics()
    
    def run(self, query: str) -> Dict[str, Any]:
        """Process an award-related query"""
        start_time = time.time()
        
        # Sanitize input
        sanitized_query, is_injection = self._sanitize_input(query)
        
        if is_injection:
            result = {
                "query": query,
                "response": "I can only provide information about Sundt Construction awards. Please rephrase your query.",
                "success": False,
                "reason": "Potential prompt injection detected"
            }
        else:
            # Search for relevant awards
            search_results = self.search_engine.search(sanitized_query, "awards", limit=5)
            awards = search_results.get("awards", [])
            
            if not awards:
                result = {
                    "query": sanitized_query,
                    "response": "I couldn't find any Sundt Construction awards matching your query. Would you like to try a different search term?",
                    "success": False,
                    "reason": "No matching awards found"
                }
            else:
                # Format award data for the prompt
                award_data = []
                for i, award in enumerate(awards, 1):
                    award_info = [f"AWARD {i}:"]
                    award_info.append(f"Title: {award.get('title', 'Untitled')}")
                    
                    for field in ["organization", "category", "award_type", "description", "location"]:
                        if field in award and award[field]:
                            award_info.append(f"{field.title()}: {award.get(field)}")
                    
                    # Add date/year if available
                    if "date" in award:
                        award_info.append(f"Date: {award.get('date')}")
                    elif "year" in award:
                        award_info.append(f"Year: {award.get('year')}")
                    
                    # Add project information if available
                    if "projects" in award and award["projects"]:
                        projects = award.get("projects")
                        if isinstance(projects, list):
                            project_titles = [p.get("title", "Unnamed Project") for p in projects]
                            award_info.append(f"Related Projects: {', '.join(project_titles)}")
                    
                    award_info.append("")
                    award_data.append("\n".join(award_info))
                
                # Generate response using LLM
                try:
                    response = self.chain.run(query=sanitized_query, award_data="\n".join(award_data))
                    
                    result = {
                        "query": sanitized_query,
                        "response": response,
                        "awards": awards,
                        "success": True
                    }
                except Exception as e:
                    result = {
                        "query": sanitized_query,
                        "response": "I encountered an error while processing your request about Sundt awards. Please try again.",
                        "success": False,
                        "reason": str(e)
                    }
        
        # Calculate execution time
        execution_time = time.time() - start_time
        result["execution_time"] = execution_time
        
        # Update metrics
        self._update_metrics(query, execution_time, is_injection)
        
        return result


# Example usage for testing
if __name__ == "__main__":
    # Create a .env file with OPENAI_API_KEY=your_api_key before running
    awards_agent = AwardsAgent()
    
    # Test queries
    test_queries = [
        "What safety awards has Sundt received?",
        "Tell me about Build America awards",
        "Has Sundt won any ENR awards in 2022?"
    ]
    
    for query in test_queries:
        print(f"\nQUERY: {query}")
        result = awards_agent.run(query)
        
        print(f"Success: {result['success']}")
        print(f"Execution time: {result['execution_time']:.2f} seconds")
        print("\nRESPONSE:")
        print(result["response"])
        print("\n" + "="*50)

================
File: backend/projects_agent.py
================
import os
import json
import re
from datetime import datetime
import time
from typing import Dict, Any, List
from search_engine import SearchEngine
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from dotenv import load_dotenv

# Load environment variables from .env file (contains OPENAI_API_KEY)
load_dotenv()

class ProjectsAgent:
    """Agent specialized for retrieving project information"""
    
    def __init__(self, model_name="gpt-3.5-turbo", temperature=0.2):
        # Initialize search engine
        self.search_engine = SearchEngine()
        
        # Initialize OpenAI LLM
        self.model_name = model_name
        self.temperature = temperature
        self.llm = ChatOpenAI(model_name=model_name, temperature=temperature)
        
        # Set up the prompt template
        self.prompt_template = PromptTemplate(
            input_variables=["query", "project_data"],
            template="""
            You are the Projects Agent for Sundt Construction. Your role is to provide 
            accurate information about Sundt's past construction projects.
            
            USER QUERY: {query}
            
            PROJECT DATA:
            {project_data}
            
            Based on the information provided, respond to the user's query about Sundt's projects.
            Present the information in a clear, concise, and helpful manner.
            If the provided data doesn't contain relevant information to answer the query,
            say that you don't have that specific information about Sundt's projects.
            
            RESPONSE:
            """
        )
        
        # Create the chain
        self.chain = LLMChain(llm=self.llm, prompt=self.prompt_template)
        
        # Set up metrics tracking
        self.metrics_file = os.path.join("data", "projects_agent_metrics.json")
        self.metrics = self._load_metrics()
    
    def _load_metrics(self) -> Dict[str, Any]:
        """Load metrics from file or initialize if not exists"""
        if os.path.exists(self.metrics_file):
            try:
                with open(self.metrics_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return self._initialize_metrics()
        else:
            return self._initialize_metrics()
    
    def _initialize_metrics(self) -> Dict[str, Any]:
        """Initialize empty metrics structure"""
        return {
            "total_queries": 0,
            "query_times": [],
            "injection_attempts": [],
            "queries_by_date": {},
            "popular_terms": {}
        }
    
    def _update_metrics(self, query: str, execution_time: float, 
                       is_injection: bool = False) -> None:
        """Update metrics with query information"""
        # Load latest metrics
        self.metrics = self._load_metrics()
        
        # Get today's date as string
        today = datetime.now().strftime("%Y-%m-%d")
        
        # Update total queries
        self.metrics["total_queries"] += 1
        
        # Update query times (keep the last 100)
        self.metrics["query_times"].append(execution_time)
        if len(self.metrics["query_times"]) > 100:
            self.metrics["query_times"] = self.metrics["query_times"][-100:]
        
        # Log injection attempts
        if is_injection:
            self.metrics["injection_attempts"].append({
                "query": query,
                "date": today
            })
        
        # Update date-based metrics
        if today not in self.metrics["queries_by_date"]:
            self.metrics["queries_by_date"][today] = 0
        self.metrics["queries_by_date"][today] += 1
        
        # Update popular terms (simple word frequency)
        words = re.findall(r'\b\w+\b', query.lower())
        for word in words:
            if len(word) > 3:  # Only count words with more than 3 characters
                if word not in self.metrics["popular_terms"]:
                    self.metrics["popular_terms"][word] = 0
                self.metrics["popular_terms"][word] += 1
        
        # Save updated metrics
        self._save_metrics()
    
    def _save_metrics(self) -> None:
        """Save metrics to file"""
        try:
            os.makedirs(os.path.dirname(self.metrics_file), exist_ok=True)
            with open(self.metrics_file, 'w', encoding='utf-8') as f:
                json.dump(self.metrics, f, indent=2)
        except Exception as e:
            print(f"Error saving metrics: {e}")
    
    def _sanitize_input(self, query: str) -> tuple:
        """
        Sanitize user input to prevent prompt injection
        Returns tuple of (sanitized_query, is_injection)
        """
        # Check for common prompt injection patterns
        injection_patterns = [
            r'ignore previous instructions',
            r'disregard (?:all|previous)',
            r'forget (?:all|your|previous)',
            r'new prompt:',
            r'system prompt:',
            r'new instructions:',
            r'you are now',
            r'you will be',
            r'your new role',
        ]
        
        for pattern in injection_patterns:
            if re.search(pattern, query, re.IGNORECASE):
                # Don't modify the query for metrics but flag as injection
                return (query, True)
        
        # Basic sanitization
        sanitized = re.sub(r'[^\w\s\.,\-\?:;\'\"()]', ' ', query)
        sanitized = re.sub(r'\s+', ' ', sanitized).strip()
        
        return (sanitized, False)
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get current metrics"""
        return self._load_metrics()
    
    def run(self, query: str) -> Dict[str, Any]:
        """Process a project-related query"""
        start_time = time.time()
        
        # Sanitize input
        sanitized_query, is_injection = self._sanitize_input(query)
        
        if is_injection:
            result = {
                "query": query,
                "response": "I can only provide information about Sundt Construction projects. Please rephrase your query.",
                "success": False,
                "reason": "Potential prompt injection detected"
            }
        else:
            # Search for relevant projects
            search_results = self.search_engine.search(sanitized_query, "projects", limit=5)
            projects = search_results.get("projects", [])
            
            if not projects:
                result = {
                    "query": sanitized_query,
                    "response": "I couldn't find any Sundt Construction projects matching your query. Would you like to try a different search term?",
                    "success": False,
                    "reason": "No matching projects found"
                }
            else:
                # Format project data for the prompt
                project_data = []
                for i, project in enumerate(projects, 1):
                    project_info = [f"PROJECT {i}:"]
                    project_info.append(f"Title: {project.get('title', 'Untitled')}")
                    
                    if "description" in project:
                        project_info.append(f"Description: {project.get('description')}")
                    elif "overview" in project:
                        project_info.append(f"Overview: {project.get('overview')}")
                    
                    for field in ["location", "client", "value"]:
                        if field in project:
                            project_info.append(f"{field.title()}: {project.get(field)}")
                    
                    if "features" in project and project["features"]:
                        features = project.get("features")
                        features_text = ", ".join(features) if isinstance(features, list) else features
                        project_info.append(f"Features: {features_text}")
                    
                    project_info.append("")
                    project_data.append("\n".join(project_info))
                
                # Generate response using LLM
                try:
                    response = self.chain.run(query=sanitized_query, project_data="\n".join(project_data))
                    
                    result = {
                        "query": sanitized_query,
                        "response": response,
                        "projects": projects,
                        "success": True
                    }
                except Exception as e:
                    result = {
                        "query": sanitized_query,
                        "response": "I encountered an error while processing your request about Sundt projects. Please try again.",
                        "success": False,
                        "reason": str(e)
                    }
        
        # Calculate execution time
        execution_time = time.time() - start_time
        result["execution_time"] = execution_time
        
        # Update metrics
        self._update_metrics(query, execution_time, is_injection)
        
        return result


# Example usage for testing
if __name__ == "__main__":
    # Create a .env file with OPENAI_API_KEY=your_api_key before running
    projects_agent = ProjectsAgent()
    
    # Test queries
    test_queries = [
        "Tell me about water treatment projects",
        "What bridge projects has Sundt completed?",
        "Show me hospital construction projects in Arizona"
    ]
    
    for query in test_queries:
        print(f"\nQUERY: {query}")
        result = projects_agent.run(query)
        
        print(f"Success: {result['success']}")
        print(f"Execution time: {result['execution_time']:.2f} seconds")
        print("\nRESPONSE:")
        print(result["response"])
        print("\n" + "="*50)

================
File: backend/requirements.txt
================
requests
beautifulsoup4
langchain
langchain-openai
openai
python-dotenv
fastapi
uvicorn
pydantic
tiktoken

================
File: backend/run_api.sh
================
#!/bin/bash
# Make sure you have installed all requirements
# pip install -r requirements.txt

# Make sure you have OpenAI API key in .env file
if [ ! -f .env ]; then
    echo "ERROR: .env file not found. Creating template .env file."
    echo "OPENAI_API_KEY=your_api_key_here" > .env
    echo "Please edit the .env file and add your OpenAI API key."
    exit 1
fi

# Check if data directory exists
if [ ! -d "data" ]; then
    echo "WARNING: Data directory not found. Creating data directory."
    mkdir -p data
fi

# Check if crawlers have been run
if [ ! -f "data/projects.json" ] || [ ! -f "data/awards.json" ]; then
    echo "WARNING: Project or award data not found."
    echo "Would you like to run the crawlers now? (y/n)"
    read -r response
    if [[ "$response" =~ ^([yY][eE][sS]|[yY])$ ]]; then
        echo "Running projects crawler..."
        python -c "from crawlers.projects_crawler import SundtProjectsCrawler; SundtProjectsCrawler().crawl()"
        echo "Running awards crawler..."
        python -c "from crawlers.awards_crawler import SundtAwardsCrawler; SundtAwardsCrawler().crawl()"
    else
        echo "Skipping crawler execution. Some API endpoints may return empty results."
    fi
fi

# Run the API server
echo "Starting Sundt RAG API server..."
python api.py

================
File: backend/search_engine.py
================
import os
import json
import re
from typing import List, Dict, Any, Union, Optional

class SearchEngine:
    """
    Simple search engine for Sundt data (projects and awards)
    Uses keyword-based search with basic relevance scoring
    """
    
    def __init__(self, data_dir="data"):
        self.data_dir = data_dir
        self.projects_file = os.path.join(data_dir, "projects.json")
        self.awards_file = os.path.join(data_dir, "awards.json")
        
        # Load data
        self.projects = self._load_json_data(self.projects_file, "projects")
        self.awards = self._load_json_data(self.awards_file, "awards")
        
        # Fields to search in for each data type
        self.project_search_fields = [
            "title", "description", "overview", "location", "client", 
            "specialties", "features", "impact.description", "testimonial"
        ]
        
        self.award_search_fields = [
            "title", "organization", "description", "location", 
            "category", "award_type", "projects.title"
        ]
    
    def _load_json_data(self, file_path: str, key: str) -> List[Dict[str, Any]]:
        """Load JSON data from file"""
        if not os.path.exists(file_path):
            print(f"Warning: {file_path} does not exist")
            return []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get(key, [])
        except Exception as e:
            print(f"Error loading data from {file_path}: {e}")
            return []
    
    def _get_nested_value(self, obj: Dict[str, Any], path: str) -> Optional[Union[str, List[str]]]:
        """
        Extract a value from a nested dictionary using dot notation
        For example: "impact.description" will get obj["impact"]["description"]
        Handles lists of objects too (e.g., projects.title)
        """
        if not path:
            return None
        
        parts = path.split('.')
        current = obj
        
        for i, part in enumerate(parts):
            if isinstance(current, dict) and part in current:
                current = current[part]
            elif isinstance(current, list):
                # If we're at the last part and it's a list of objects
                if i == len(parts) - 1:
                    values = []
                    for item in current:
                        if isinstance(item, dict) and part in item:
                            values.append(str(item[part]))
                    return " ".join(values) if values else None
                return None
            else:
                return None
        
        # Convert to string if it's a simple value, or join list items
        if isinstance(current, list):
            return " ".join(str(item) for item in current)
        return str(current) if current is not None else None
    
    def search_projects(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Search for projects matching the query"""
        if not query or not self.projects:
            return []
        
        # Tokenize query and create regex patterns
        query_terms = re.findall(r'\w+', query.lower())
        if not query_terms:
            return []
        
        # Score each project
        scored_projects = []
        for project in self.projects:
            score = 0
            matches = []
            
            for field in self.project_search_fields:
                value = self._get_nested_value(project, field)
                if not value:
                    continue
                
                # Score based on number of term matches
                value_lower = value.lower()
                field_matches = 0
                
                for term in query_terms:
                    # Count instances of the term
                    term_count = len(re.findall(r'\b' + re.escape(term) + r'\b', value_lower))
                    if term_count > 0:
                        field_matches += term_count
                        
                        # Record which field matched for explanation
                        matches.append(f"{field}: '{term}'")
                
                # Add to score, weigh certain fields more
                if field == "title":
                    score += field_matches * 3  # Title matches are more important
                elif field in ["description", "overview"]:
                    score += field_matches * 2  # Description matches are also important
                else:
                    score += field_matches
            
            if score > 0:
                scored_projects.append({
                    "project": project,
                    "score": score,
                    "matches": matches
                })
        
        # Sort by score (descending) and return projects
        scored_projects.sort(key=lambda x: x["score"], reverse=True)
        return [item["project"] for item in scored_projects[:limit]]
    
    def search_awards(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Search for awards matching the query"""
        if not query or not self.awards:
            return []
        
        # Tokenize query and create regex patterns
        query_terms = re.findall(r'\w+', query.lower())
        if not query_terms:
            return []
        
        # Score each award
        scored_awards = []
        for award in self.awards:
            score = 0
            matches = []
            
            for field in self.award_search_fields:
                value = self._get_nested_value(award, field)
                if not value:
                    continue
                
                # Score based on number of term matches
                value_lower = value.lower()
                field_matches = 0
                
                for term in query_terms:
                    # Count instances of the term
                    term_count = len(re.findall(r'\b' + re.escape(term) + r'\b', value_lower))
                    if term_count > 0:
                        field_matches += term_count
                        
                        # Record which field matched for explanation
                        matches.append(f"{field}: '{term}'")
                
                # Add to score, weigh certain fields more
                if field == "title":
                    score += field_matches * 3  # Title matches are more important
                elif field in ["organization", "category", "award_type"]:
                    score += field_matches * 2  # These matches are also important
                else:
                    score += field_matches
            
            if score > 0:
                scored_awards.append({
                    "award": award,
                    "score": score,
                    "matches": matches
                })
        
        # Sort by score (descending) and return awards
        scored_awards.sort(key=lambda x: x["score"], reverse=True)
        return [item["award"] for item in scored_awards[:limit]]
    
    def search(self, query: str, type: str = "all", limit: int = 10) -> Dict[str, Union[List[Dict[str, Any]], int]]:
        """
        Search for projects and/or awards matching the query
        
        Args:
            query: Search terms
            type: "projects", "awards", or "all"
            limit: Maximum number of results to return for each type
            
        Returns:
            Dictionary with results and timing info
        """
        results = {
            "query": query,
            "type": type,
        }
        
        if type == "projects" or type == "all":
            projects = self.search_projects(query, limit)
            results["projects"] = projects
            results["project_count"] = len(projects)
            
        if type == "awards" or type == "all":
            awards = self.search_awards(query, limit)
            results["awards"] = awards
            results["award_count"] = len(awards)
            
        return results


# Example usage
if __name__ == "__main__":
    engine = SearchEngine()
    
    # Example project search
    project_results = engine.search("water treatment", "projects", 5)
    print(f"Found {project_results['project_count']} matching projects")
    for i, project in enumerate(project_results.get("projects", [])):
        print(f"{i+1}. {project.get('title', 'Untitled')}")
        print(f"   {project.get('description', '')[:100]}...\n")
    
    # Example award search
    award_results = engine.search("safety excellence", "awards", 5)
    print(f"Found {award_results['award_count']} matching awards")
    for i, award in enumerate(award_results.get("awards", [])):
        print(f"{i+1}. {award.get('title', 'Untitled')}")
        print(f"   {award.get('organization', '')}")
        print(f"   {award.get('description', '')[:100]}...\n")

================
File: backend/sundt_cli.py
================
import os
import dotenv
from projects_agent import ProjectsAgent
from awards_agent import AwardsAgent
import time

# Load environment variables
dotenv.load_dotenv()

class SundtCLI:
    def __init__(self):
        print("Initializing Sundt RAG CLI...")
        print("Loading agents...")
        self.projects_agent = ProjectsAgent()
        self.awards_agent = AwardsAgent()
        print("Agents loaded successfully!")
    
    def run(self):
        """Run the interactive CLI"""
        print("\n" + "="*60)
        print("               SUNDT CONSTRUCTION RAG SYSTEM")
        print("="*60)
        print("Welcome to the Sundt Construction RAG interface.")
        print("This system provides information about Sundt's projects and awards.")
        print("\nAvailable commands:")
        print("  projects <query> - Search for information about Sundt projects")
        print("  awards <query>   - Search for information about Sundt awards")
        print("  help             - Show this help message")
        print("  exit             - Exit the application")
        print("="*60)
        
        while True:
            try:
                user_input = input("\nEnter your command: ").strip()
                
                if not user_input:
                    continue
                    
                if user_input.lower() == "exit":
                    print("Thank you for using the Sundt RAG system. Goodbye!")
                    break
                    
                if user_input.lower() == "help":
                    print("\nAvailable commands:")
                    print("  projects <query> - Search for information about Sundt projects")
                    print("  awards <query>   - Search for information about Sundt awards")
                    print("  help             - Show this help message")
                    print("  exit             - Exit the application")
                    continue
                
                # Parse command and query
                parts = user_input.split(maxsplit=1)
                if len(parts) < 2:
                    print("Please provide a query after the command. Type 'help' for more information.")
                    continue
                    
                command, query = parts
                
                if command.lower() == "projects":
                    self._handle_projects_query(query)
                elif command.lower() == "awards":
                    self._handle_awards_query(query)
                else:
                    print(f"Unknown command: {command}")
                    print("Type 'help' to see available commands.")
            
            except KeyboardInterrupt:
                print("\nOperation cancelled by user.")
                break
            except Exception as e:
                print(f"Error: {str(e)}")
    
    def _handle_projects_query(self, query):
        """Process a query for the Projects agent"""
        print(f"\nSearching for projects related to: {query}")
        print("Processing...")
        
        start_time = time.time()
        result = self.projects_agent.run(query)
        duration = time.time() - start_time
        
        if result["success"]:
            print(f"\nFound {len(result.get('projects', []))} relevant projects")
            print(f"Response time: {duration:.2f} seconds")
            print("\nRESPONSE:")
            print(result["response"])
            
            # List the projects found
            if result.get("projects"):
                print("\nProjects found:")
                for i, project in enumerate(result["projects"], 1):
                    print(f"  {i}. {project.get('title', 'Untitled')}")
        else:
            print(f"\nNo results found. Reason: {result.get('reason', 'Unknown')}")
            print(f"Response: {result['response']}")
    
    def _handle_awards_query(self, query):
        """Process a query for the Awards agent"""
        print(f"\nSearching for awards related to: {query}")
        print("Processing...")
        
        start_time = time.time()
        result = self.awards_agent.run(query)
        duration = time.time() - start_time
        
        if result["success"]:
            print(f"\nFound {len(result.get('awards', []))} relevant awards")
            print(f"Response time: {duration:.2f} seconds")
            print("\nRESPONSE:")
            print(result["response"])
            
            # List the awards found
            if result.get("awards"):
                print("\nAwards found:")
                for i, award in enumerate(result["awards"], 1):
                    title = award.get('title', 'Untitled')
                    org = f" ({award.get('organization', '')})" if 'organization' in award else ""
                    print(f"  {i}. {title}{org}")
        else:
            print(f"\nNo results found. Reason: {result.get('reason', 'Unknown')}")
            print(f"Response: {result['response']}")

if __name__ == "__main__":
    # Check if OpenAI API key is set
    if not os.getenv("OPENAI_API_KEY"):
        print("ERROR: OPENAI_API_KEY environment variable not set.")
        print("Please create a .env file with your OpenAI API key:")
        print("OPENAI_API_KEY=your_api_key_here")
        exit(1)
    
    # Start the CLI
    cli = SundtCLI()
    cli.run()

================
File: backend/test_search_engine.py
================
import sys
import json
from search_engine import SearchEngine

def test_search_engine():
    # Initialize the search engine
    engine = SearchEngine()
    
    # Print counts of loaded data
    print(f"Loaded {len(engine.projects)} projects")
    print(f"Loaded {len(engine.awards)} awards")
    
    # Test project searches
    print("\n--- PROJECT SEARCH TESTS ---")
    test_project_queries = [
        "water treatment",
        "transportation",
        "hospital",
        "bridge",
        "San Antonio"
    ]
    
    for query in test_project_queries:
        print(f"\nSearch query: '{query}'")
        results = engine.search_projects(query, limit=3)
        print(f"Found {len(results)} matching projects")
        
        for i, project in enumerate(results):
            print(f"  {i+1}. {project.get('title', 'Untitled')}")
            # Print a brief snippet of the description if available
            if "description" in project:
                desc = project["description"]
                print(f"     {desc[:100]}..." if len(desc) > 100 else f"     {desc}")
            elif "overview" in project:
                overview = project["overview"]
                print(f"     {overview[:100]}..." if len(overview) > 100 else f"     {overview}")
    
    # Test award searches
    print("\n--- AWARD SEARCH TESTS ---")
    test_award_queries = [
        "safety",
        "excellence",
        "build america",
        "ENR",
        "2022"
    ]
    
    for query in test_award_queries:
        print(f"\nSearch query: '{query}'")
        results = engine.search_awards(query, limit=3)
        print(f"Found {len(results)} matching awards")
        
        for i, award in enumerate(results):
            print(f"  {i+1}. {award.get('title', 'Untitled')}")
            if "organization" in award:
                print(f"     Organization: {award['organization']}")
            if "description" in award:
                desc = award["description"]
                print(f"     {desc[:100]}..." if len(desc) > 100 else f"     {desc}")
    
    # Test combined search
    print("\n--- COMBINED SEARCH TEST ---")
    combined_query = "bridge construction safety"
    print(f"Search query: '{combined_query}'")
    results = engine.search(combined_query, "all", limit=3)
    
    print(f"Found {results.get('project_count', 0)} matching projects")
    for i, project in enumerate(results.get('projects', [])):
        print(f"  Project {i+1}: {project.get('title', 'Untitled')}")
    
    print(f"Found {results.get('award_count', 0)} matching awards")
    for i, award in enumerate(results.get('awards', [])):
        print(f"  Award {i+1}: {award.get('title', 'Untitled')}")

if __name__ == "__main__":
    test_search_engine()

================
File: frontend/.git/hooks/applypatch-msg.sample
================
#!/bin/sh
#
# An example hook script to check the commit log message taken by
# applypatch from an e-mail message.
#
# The hook should exit with non-zero status after issuing an
# appropriate message if it wants to stop the commit.  The hook is
# allowed to edit the commit message file.
#
# To enable this hook, rename this file to "applypatch-msg".

. git-sh-setup
commitmsg="$(git rev-parse --git-path hooks/commit-msg)"
test -x "$commitmsg" && exec "$commitmsg" ${1+"$@"}
:

================
File: frontend/.git/hooks/commit-msg.sample
================
#!/bin/sh
#
# An example hook script to check the commit log message.
# Called by "git commit" with one argument, the name of the file
# that has the commit message.  The hook should exit with non-zero
# status after issuing an appropriate message if it wants to stop the
# commit.  The hook is allowed to edit the commit message file.
#
# To enable this hook, rename this file to "commit-msg".

# Uncomment the below to add a Signed-off-by line to the message.
# Doing this in a hook is a bad idea in general, but the prepare-commit-msg
# hook is more suited to it.
#
# SOB=$(git var GIT_AUTHOR_IDENT | sed -n 's/^\(.*>\).*$/Signed-off-by: \1/p')
# grep -qs "^$SOB" "$1" || echo "$SOB" >> "$1"

# This example catches duplicate Signed-off-by lines.

test "" = "$(grep '^Signed-off-by: ' "$1" |
	 sort | uniq -c | sed -e '/^[ 	]*1[ 	]/d')" || {
	echo >&2 Duplicate Signed-off-by lines.
	exit 1
}

================
File: frontend/.git/hooks/fsmonitor-watchman.sample
================
#!/usr/bin/perl

use strict;
use warnings;
use IPC::Open2;

# An example hook script to integrate Watchman
# (https://facebook.github.io/watchman/) with git to speed up detecting
# new and modified files.
#
# The hook is passed a version (currently 2) and last update token
# formatted as a string and outputs to stdout a new update token and
# all files that have been modified since the update token. Paths must
# be relative to the root of the working tree and separated by a single NUL.
#
# To enable this hook, rename this file to "query-watchman" and set
# 'git config core.fsmonitor .git/hooks/query-watchman'
#
my ($version, $last_update_token) = @ARGV;

# Uncomment for debugging
# print STDERR "$0 $version $last_update_token\n";

# Check the hook interface version
if ($version ne 2) {
	die "Unsupported query-fsmonitor hook version '$version'.\n" .
	    "Falling back to scanning...\n";
}

my $git_work_tree = get_working_dir();

my $retry = 1;

my $json_pkg;
eval {
	require JSON::XS;
	$json_pkg = "JSON::XS";
	1;
} or do {
	require JSON::PP;
	$json_pkg = "JSON::PP";
};

launch_watchman();

sub launch_watchman {
	my $o = watchman_query();
	if (is_work_tree_watched($o)) {
		output_result($o->{clock}, @{$o->{files}});
	}
}

sub output_result {
	my ($clockid, @files) = @_;

	# Uncomment for debugging watchman output
	# open (my $fh, ">", ".git/watchman-output.out");
	# binmode $fh, ":utf8";
	# print $fh "$clockid\n@files\n";
	# close $fh;

	binmode STDOUT, ":utf8";
	print $clockid;
	print "\0";
	local $, = "\0";
	print @files;
}

sub watchman_clock {
	my $response = qx/watchman clock "$git_work_tree"/;
	die "Failed to get clock id on '$git_work_tree'.\n" .
		"Falling back to scanning...\n" if $? != 0;

	return $json_pkg->new->utf8->decode($response);
}

sub watchman_query {
	my $pid = open2(\*CHLD_OUT, \*CHLD_IN, 'watchman -j --no-pretty')
	or die "open2() failed: $!\n" .
	"Falling back to scanning...\n";

	# In the query expression below we're asking for names of files that
	# changed since $last_update_token but not from the .git folder.
	#
	# To accomplish this, we're using the "since" generator to use the
	# recency index to select candidate nodes and "fields" to limit the
	# output to file names only. Then we're using the "expression" term to
	# further constrain the results.
	my $last_update_line = "";
	if (substr($last_update_token, 0, 1) eq "c") {
		$last_update_token = "\"$last_update_token\"";
		$last_update_line = qq[\n"since": $last_update_token,];
	}
	my $query = <<"	END";
		["query", "$git_work_tree", {$last_update_line
			"fields": ["name"],
			"expression": ["not", ["dirname", ".git"]]
		}]
	END

	# Uncomment for debugging the watchman query
	# open (my $fh, ">", ".git/watchman-query.json");
	# print $fh $query;
	# close $fh;

	print CHLD_IN $query;
	close CHLD_IN;
	my $response = do {local $/; <CHLD_OUT>};

	# Uncomment for debugging the watch response
	# open ($fh, ">", ".git/watchman-response.json");
	# print $fh $response;
	# close $fh;

	die "Watchman: command returned no output.\n" .
	"Falling back to scanning...\n" if $response eq "";
	die "Watchman: command returned invalid output: $response\n" .
	"Falling back to scanning...\n" unless $response =~ /^\{/;

	return $json_pkg->new->utf8->decode($response);
}

sub is_work_tree_watched {
	my ($output) = @_;
	my $error = $output->{error};
	if ($retry > 0 and $error and $error =~ m/unable to resolve root .* directory (.*) is not watched/) {
		$retry--;
		my $response = qx/watchman watch "$git_work_tree"/;
		die "Failed to make watchman watch '$git_work_tree'.\n" .
		    "Falling back to scanning...\n" if $? != 0;
		$output = $json_pkg->new->utf8->decode($response);
		$error = $output->{error};
		die "Watchman: $error.\n" .
		"Falling back to scanning...\n" if $error;

		# Uncomment for debugging watchman output
		# open (my $fh, ">", ".git/watchman-output.out");
		# close $fh;

		# Watchman will always return all files on the first query so
		# return the fast "everything is dirty" flag to git and do the
		# Watchman query just to get it over with now so we won't pay
		# the cost in git to look up each individual file.
		my $o = watchman_clock();
		$error = $output->{error};

		die "Watchman: $error.\n" .
		"Falling back to scanning...\n" if $error;

		output_result($o->{clock}, ("/"));
		$last_update_token = $o->{clock};

		eval { launch_watchman() };
		return 0;
	}

	die "Watchman: $error.\n" .
	"Falling back to scanning...\n" if $error;

	return 1;
}

sub get_working_dir {
	my $working_dir;
	if ($^O =~ 'msys' || $^O =~ 'cygwin') {
		$working_dir = Win32::GetCwd();
		$working_dir =~ tr/\\/\//;
	} else {
		require Cwd;
		$working_dir = Cwd::cwd();
	}

	return $working_dir;
}

================
File: frontend/.git/hooks/post-update.sample
================
#!/bin/sh
#
# An example hook script to prepare a packed repository for use over
# dumb transports.
#
# To enable this hook, rename this file to "post-update".

exec git update-server-info

================
File: frontend/.git/hooks/pre-applypatch.sample
================
#!/bin/sh
#
# An example hook script to verify what is about to be committed
# by applypatch from an e-mail message.
#
# The hook should exit with non-zero status after issuing an
# appropriate message if it wants to stop the commit.
#
# To enable this hook, rename this file to "pre-applypatch".

. git-sh-setup
precommit="$(git rev-parse --git-path hooks/pre-commit)"
test -x "$precommit" && exec "$precommit" ${1+"$@"}
:

================
File: frontend/.git/hooks/pre-commit.sample
================
#!/bin/sh
#
# An example hook script to verify what is about to be committed.
# Called by "git commit" with no arguments.  The hook should
# exit with non-zero status after issuing an appropriate message if
# it wants to stop the commit.
#
# To enable this hook, rename this file to "pre-commit".

if git rev-parse --verify HEAD >/dev/null 2>&1
then
	against=HEAD
else
	# Initial commit: diff against an empty tree object
	against=$(git hash-object -t tree /dev/null)
fi

# If you want to allow non-ASCII filenames set this variable to true.
allownonascii=$(git config --type=bool hooks.allownonascii)

# Redirect output to stderr.
exec 1>&2

# Cross platform projects tend to avoid non-ASCII filenames; prevent
# them from being added to the repository. We exploit the fact that the
# printable range starts at the space character and ends with tilde.
if [ "$allownonascii" != "true" ] &&
	# Note that the use of brackets around a tr range is ok here, (it's
	# even required, for portability to Solaris 10's /usr/bin/tr), since
	# the square bracket bytes happen to fall in the designated range.
	test $(git diff --cached --name-only --diff-filter=A -z $against |
	  LC_ALL=C tr -d '[ -~]\0' | wc -c) != 0
then
	cat <<\EOF
Error: Attempt to add a non-ASCII file name.

This can cause problems if you want to work with people on other platforms.

To be portable it is advisable to rename the file.

If you know what you are doing you can disable this check using:

  git config hooks.allownonascii true
EOF
	exit 1
fi

# If there are whitespace errors, print the offending file names and fail.
exec git diff-index --check --cached $against --

================
File: frontend/.git/hooks/pre-merge-commit.sample
================
#!/bin/sh
#
# An example hook script to verify what is about to be committed.
# Called by "git merge" with no arguments.  The hook should
# exit with non-zero status after issuing an appropriate message to
# stderr if it wants to stop the merge commit.
#
# To enable this hook, rename this file to "pre-merge-commit".

. git-sh-setup
test -x "$GIT_DIR/hooks/pre-commit" &&
        exec "$GIT_DIR/hooks/pre-commit"
:

================
File: frontend/.git/hooks/pre-push.sample
================
#!/bin/sh

# An example hook script to verify what is about to be pushed.  Called by "git
# push" after it has checked the remote status, but before anything has been
# pushed.  If this script exits with a non-zero status nothing will be pushed.
#
# This hook is called with the following parameters:
#
# $1 -- Name of the remote to which the push is being done
# $2 -- URL to which the push is being done
#
# If pushing without using a named remote those arguments will be equal.
#
# Information about the commits which are being pushed is supplied as lines to
# the standard input in the form:
#
#   <local ref> <local oid> <remote ref> <remote oid>
#
# This sample shows how to prevent push of commits where the log message starts
# with "WIP" (work in progress).

remote="$1"
url="$2"

zero=$(git hash-object --stdin </dev/null | tr '[0-9a-f]' '0')

while read local_ref local_oid remote_ref remote_oid
do
	if test "$local_oid" = "$zero"
	then
		# Handle delete
		:
	else
		if test "$remote_oid" = "$zero"
		then
			# New branch, examine all commits
			range="$local_oid"
		else
			# Update to existing branch, examine new commits
			range="$remote_oid..$local_oid"
		fi

		# Check for WIP commit
		commit=$(git rev-list -n 1 --grep '^WIP' "$range")
		if test -n "$commit"
		then
			echo >&2 "Found WIP commit in $local_ref, not pushing"
			exit 1
		fi
	fi
done

exit 0

================
File: frontend/.git/hooks/pre-rebase.sample
================
#!/bin/sh
#
# Copyright (c) 2006, 2008 Junio C Hamano
#
# The "pre-rebase" hook is run just before "git rebase" starts doing
# its job, and can prevent the command from running by exiting with
# non-zero status.
#
# The hook is called with the following parameters:
#
# $1 -- the upstream the series was forked from.
# $2 -- the branch being rebased (or empty when rebasing the current branch).
#
# This sample shows how to prevent topic branches that are already
# merged to 'next' branch from getting rebased, because allowing it
# would result in rebasing already published history.

publish=next
basebranch="$1"
if test "$#" = 2
then
	topic="refs/heads/$2"
else
	topic=`git symbolic-ref HEAD` ||
	exit 0 ;# we do not interrupt rebasing detached HEAD
fi

case "$topic" in
refs/heads/??/*)
	;;
*)
	exit 0 ;# we do not interrupt others.
	;;
esac

# Now we are dealing with a topic branch being rebased
# on top of master.  Is it OK to rebase it?

# Does the topic really exist?
git show-ref -q "$topic" || {
	echo >&2 "No such branch $topic"
	exit 1
}

# Is topic fully merged to master?
not_in_master=`git rev-list --pretty=oneline ^master "$topic"`
if test -z "$not_in_master"
then
	echo >&2 "$topic is fully merged to master; better remove it."
	exit 1 ;# we could allow it, but there is no point.
fi

# Is topic ever merged to next?  If so you should not be rebasing it.
only_next_1=`git rev-list ^master "^$topic" ${publish} | sort`
only_next_2=`git rev-list ^master           ${publish} | sort`
if test "$only_next_1" = "$only_next_2"
then
	not_in_topic=`git rev-list "^$topic" master`
	if test -z "$not_in_topic"
	then
		echo >&2 "$topic is already up to date with master"
		exit 1 ;# we could allow it, but there is no point.
	else
		exit 0
	fi
else
	not_in_next=`git rev-list --pretty=oneline ^${publish} "$topic"`
	/usr/bin/perl -e '
		my $topic = $ARGV[0];
		my $msg = "* $topic has commits already merged to public branch:\n";
		my (%not_in_next) = map {
			/^([0-9a-f]+) /;
			($1 => 1);
		} split(/\n/, $ARGV[1]);
		for my $elem (map {
				/^([0-9a-f]+) (.*)$/;
				[$1 => $2];
			} split(/\n/, $ARGV[2])) {
			if (!exists $not_in_next{$elem->[0]}) {
				if ($msg) {
					print STDERR $msg;
					undef $msg;
				}
				print STDERR " $elem->[1]\n";
			}
		}
	' "$topic" "$not_in_next" "$not_in_master"
	exit 1
fi

<<\DOC_END

This sample hook safeguards topic branches that have been
published from being rewound.

The workflow assumed here is:

 * Once a topic branch forks from "master", "master" is never
   merged into it again (either directly or indirectly).

 * Once a topic branch is fully cooked and merged into "master",
   it is deleted.  If you need to build on top of it to correct
   earlier mistakes, a new topic branch is created by forking at
   the tip of the "master".  This is not strictly necessary, but
   it makes it easier to keep your history simple.

 * Whenever you need to test or publish your changes to topic
   branches, merge them into "next" branch.

The script, being an example, hardcodes the publish branch name
to be "next", but it is trivial to make it configurable via
$GIT_DIR/config mechanism.

With this workflow, you would want to know:

(1) ... if a topic branch has ever been merged to "next".  Young
    topic branches can have stupid mistakes you would rather
    clean up before publishing, and things that have not been
    merged into other branches can be easily rebased without
    affecting other people.  But once it is published, you would
    not want to rewind it.

(2) ... if a topic branch has been fully merged to "master".
    Then you can delete it.  More importantly, you should not
    build on top of it -- other people may already want to
    change things related to the topic as patches against your
    "master", so if you need further changes, it is better to
    fork the topic (perhaps with the same name) afresh from the
    tip of "master".

Let's look at this example:

		   o---o---o---o---o---o---o---o---o---o "next"
		  /       /           /           /
		 /   a---a---b A     /           /
		/   /               /           /
	       /   /   c---c---c---c B         /
	      /   /   /             \         /
	     /   /   /   b---b C     \       /
	    /   /   /   /             \     /
    ---o---o---o---o---o---o---o---o---o---o---o "master"


A, B and C are topic branches.

 * A has one fix since it was merged up to "next".

 * B has finished.  It has been fully merged up to "master" and "next",
   and is ready to be deleted.

 * C has not merged to "next" at all.

We would want to allow C to be rebased, refuse A, and encourage
B to be deleted.

To compute (1):

	git rev-list ^master ^topic next
	git rev-list ^master        next

	if these match, topic has not merged in next at all.

To compute (2):

	git rev-list master..topic

	if this is empty, it is fully merged to "master".

DOC_END

================
File: frontend/.git/hooks/pre-receive.sample
================
#!/bin/sh
#
# An example hook script to make use of push options.
# The example simply echoes all push options that start with 'echoback='
# and rejects all pushes when the "reject" push option is used.
#
# To enable this hook, rename this file to "pre-receive".

if test -n "$GIT_PUSH_OPTION_COUNT"
then
	i=0
	while test "$i" -lt "$GIT_PUSH_OPTION_COUNT"
	do
		eval "value=\$GIT_PUSH_OPTION_$i"
		case "$value" in
		echoback=*)
			echo "echo from the pre-receive-hook: ${value#*=}" >&2
			;;
		reject)
			exit 1
		esac
		i=$((i + 1))
	done
fi

================
File: frontend/.git/hooks/prepare-commit-msg.sample
================
#!/bin/sh
#
# An example hook script to prepare the commit log message.
# Called by "git commit" with the name of the file that has the
# commit message, followed by the description of the commit
# message's source.  The hook's purpose is to edit the commit
# message file.  If the hook fails with a non-zero status,
# the commit is aborted.
#
# To enable this hook, rename this file to "prepare-commit-msg".

# This hook includes three examples. The first one removes the
# "# Please enter the commit message..." help message.
#
# The second includes the output of "git diff --name-status -r"
# into the message, just before the "git status" output.  It is
# commented because it doesn't cope with --amend or with squashed
# commits.
#
# The third example adds a Signed-off-by line to the message, that can
# still be edited.  This is rarely a good idea.

COMMIT_MSG_FILE=$1
COMMIT_SOURCE=$2
SHA1=$3

/usr/bin/perl -i.bak -ne 'print unless(m/^. Please enter the commit message/..m/^#$/)' "$COMMIT_MSG_FILE"

# case "$COMMIT_SOURCE,$SHA1" in
#  ,|template,)
#    /usr/bin/perl -i.bak -pe '
#       print "\n" . `git diff --cached --name-status -r`
# 	 if /^#/ && $first++ == 0' "$COMMIT_MSG_FILE" ;;
#  *) ;;
# esac

# SOB=$(git var GIT_COMMITTER_IDENT | sed -n 's/^\(.*>\).*$/Signed-off-by: \1/p')
# git interpret-trailers --in-place --trailer "$SOB" "$COMMIT_MSG_FILE"
# if test -z "$COMMIT_SOURCE"
# then
#   /usr/bin/perl -i.bak -pe 'print "\n" if !$first_line++' "$COMMIT_MSG_FILE"
# fi

================
File: frontend/.git/hooks/push-to-checkout.sample
================
#!/bin/sh

# An example hook script to update a checked-out tree on a git push.
#
# This hook is invoked by git-receive-pack(1) when it reacts to git
# push and updates reference(s) in its repository, and when the push
# tries to update the branch that is currently checked out and the
# receive.denyCurrentBranch configuration variable is set to
# updateInstead.
#
# By default, such a push is refused if the working tree and the index
# of the remote repository has any difference from the currently
# checked out commit; when both the working tree and the index match
# the current commit, they are updated to match the newly pushed tip
# of the branch. This hook is to be used to override the default
# behaviour; however the code below reimplements the default behaviour
# as a starting point for convenient modification.
#
# The hook receives the commit with which the tip of the current
# branch is going to be updated:
commit=$1

# It can exit with a non-zero status to refuse the push (when it does
# so, it must not modify the index or the working tree).
die () {
	echo >&2 "$*"
	exit 1
}

# Or it can make any necessary changes to the working tree and to the
# index to bring them to the desired state when the tip of the current
# branch is updated to the new commit, and exit with a zero status.
#
# For example, the hook can simply run git read-tree -u -m HEAD "$1"
# in order to emulate git fetch that is run in the reverse direction
# with git push, as the two-tree form of git read-tree -u -m is
# essentially the same as git switch or git checkout that switches
# branches while keeping the local changes in the working tree that do
# not interfere with the difference between the branches.

# The below is a more-or-less exact translation to shell of the C code
# for the default behaviour for git's push-to-checkout hook defined in
# the push_to_deploy() function in builtin/receive-pack.c.
#
# Note that the hook will be executed from the repository directory,
# not from the working tree, so if you want to perform operations on
# the working tree, you will have to adapt your code accordingly, e.g.
# by adding "cd .." or using relative paths.

if ! git update-index -q --ignore-submodules --refresh
then
	die "Up-to-date check failed"
fi

if ! git diff-files --quiet --ignore-submodules --
then
	die "Working directory has unstaged changes"
fi

# This is a rough translation of:
#
#   head_has_history() ? "HEAD" : EMPTY_TREE_SHA1_HEX
if git cat-file -e HEAD 2>/dev/null
then
	head=HEAD
else
	head=$(git hash-object -t tree --stdin </dev/null)
fi

if ! git diff-index --quiet --cached --ignore-submodules $head --
then
	die "Working directory has staged changes"
fi

if ! git read-tree -u -m "$commit"
then
	die "Could not update working tree to new HEAD"
fi

================
File: frontend/.git/hooks/sendemail-validate.sample
================
#!/bin/sh

# An example hook script to validate a patch (and/or patch series) before
# sending it via email.
#
# The hook should exit with non-zero status after issuing an appropriate
# message if it wants to prevent the email(s) from being sent.
#
# To enable this hook, rename this file to "sendemail-validate".
#
# By default, it will only check that the patch(es) can be applied on top of
# the default upstream branch without conflicts in a secondary worktree. After
# validation (successful or not) of the last patch of a series, the worktree
# will be deleted.
#
# The following config variables can be set to change the default remote and
# remote ref that are used to apply the patches against:
#
#   sendemail.validateRemote (default: origin)
#   sendemail.validateRemoteRef (default: HEAD)
#
# Replace the TODO placeholders with appropriate checks according to your
# needs.

validate_cover_letter () {
	file="$1"
	# TODO: Replace with appropriate checks (e.g. spell checking).
	true
}

validate_patch () {
	file="$1"
	# Ensure that the patch applies without conflicts.
	git am -3 "$file" || return
	# TODO: Replace with appropriate checks for this patch
	# (e.g. checkpatch.pl).
	true
}

validate_series () {
	# TODO: Replace with appropriate checks for the whole series
	# (e.g. quick build, coding style checks, etc.).
	true
}

# main -------------------------------------------------------------------------

if test "$GIT_SENDEMAIL_FILE_COUNTER" = 1
then
	remote=$(git config --default origin --get sendemail.validateRemote) &&
	ref=$(git config --default HEAD --get sendemail.validateRemoteRef) &&
	worktree=$(mktemp --tmpdir -d sendemail-validate.XXXXXXX) &&
	git worktree add -fd --checkout "$worktree" "refs/remotes/$remote/$ref" &&
	git config --replace-all sendemail.validateWorktree "$worktree"
else
	worktree=$(git config --get sendemail.validateWorktree)
fi || {
	echo "sendemail-validate: error: failed to prepare worktree" >&2
	exit 1
}

unset GIT_DIR GIT_WORK_TREE
cd "$worktree" &&

if grep -q "^diff --git " "$1"
then
	validate_patch "$1"
else
	validate_cover_letter "$1"
fi &&

if test "$GIT_SENDEMAIL_FILE_COUNTER" = "$GIT_SENDEMAIL_FILE_TOTAL"
then
	git config --unset-all sendemail.validateWorktree &&
	trap 'git worktree remove -ff "$worktree"' EXIT &&
	validate_series
fi

================
File: frontend/.git/hooks/update.sample
================
#!/bin/sh
#
# An example hook script to block unannotated tags from entering.
# Called by "git receive-pack" with arguments: refname sha1-old sha1-new
#
# To enable this hook, rename this file to "update".
#
# Config
# ------
# hooks.allowunannotated
#   This boolean sets whether unannotated tags will be allowed into the
#   repository.  By default they won't be.
# hooks.allowdeletetag
#   This boolean sets whether deleting tags will be allowed in the
#   repository.  By default they won't be.
# hooks.allowmodifytag
#   This boolean sets whether a tag may be modified after creation. By default
#   it won't be.
# hooks.allowdeletebranch
#   This boolean sets whether deleting branches will be allowed in the
#   repository.  By default they won't be.
# hooks.denycreatebranch
#   This boolean sets whether remotely creating branches will be denied
#   in the repository.  By default this is allowed.
#

# --- Command line
refname="$1"
oldrev="$2"
newrev="$3"

# --- Safety check
if [ -z "$GIT_DIR" ]; then
	echo "Don't run this script from the command line." >&2
	echo " (if you want, you could supply GIT_DIR then run" >&2
	echo "  $0 <ref> <oldrev> <newrev>)" >&2
	exit 1
fi

if [ -z "$refname" -o -z "$oldrev" -o -z "$newrev" ]; then
	echo "usage: $0 <ref> <oldrev> <newrev>" >&2
	exit 1
fi

# --- Config
allowunannotated=$(git config --type=bool hooks.allowunannotated)
allowdeletebranch=$(git config --type=bool hooks.allowdeletebranch)
denycreatebranch=$(git config --type=bool hooks.denycreatebranch)
allowdeletetag=$(git config --type=bool hooks.allowdeletetag)
allowmodifytag=$(git config --type=bool hooks.allowmodifytag)

# check for no description
projectdesc=$(sed -e '1q' "$GIT_DIR/description")
case "$projectdesc" in
"Unnamed repository"* | "")
	echo "*** Project description file hasn't been set" >&2
	exit 1
	;;
esac

# --- Check types
# if $newrev is 0000...0000, it's a commit to delete a ref.
zero=$(git hash-object --stdin </dev/null | tr '[0-9a-f]' '0')
if [ "$newrev" = "$zero" ]; then
	newrev_type=delete
else
	newrev_type=$(git cat-file -t $newrev)
fi

case "$refname","$newrev_type" in
	refs/tags/*,commit)
		# un-annotated tag
		short_refname=${refname##refs/tags/}
		if [ "$allowunannotated" != "true" ]; then
			echo "*** The un-annotated tag, $short_refname, is not allowed in this repository" >&2
			echo "*** Use 'git tag [ -a | -s ]' for tags you want to propagate." >&2
			exit 1
		fi
		;;
	refs/tags/*,delete)
		# delete tag
		if [ "$allowdeletetag" != "true" ]; then
			echo "*** Deleting a tag is not allowed in this repository" >&2
			exit 1
		fi
		;;
	refs/tags/*,tag)
		# annotated tag
		if [ "$allowmodifytag" != "true" ] && git rev-parse $refname > /dev/null 2>&1
		then
			echo "*** Tag '$refname' already exists." >&2
			echo "*** Modifying a tag is not allowed in this repository." >&2
			exit 1
		fi
		;;
	refs/heads/*,commit)
		# branch
		if [ "$oldrev" = "$zero" -a "$denycreatebranch" = "true" ]; then
			echo "*** Creating a branch is not allowed in this repository" >&2
			exit 1
		fi
		;;
	refs/heads/*,delete)
		# delete branch
		if [ "$allowdeletebranch" != "true" ]; then
			echo "*** Deleting a branch is not allowed in this repository" >&2
			exit 1
		fi
		;;
	refs/remotes/*,commit)
		# tracking branch
		;;
	refs/remotes/*,delete)
		# delete tracking branch
		if [ "$allowdeletebranch" != "true" ]; then
			echo "*** Deleting a tracking branch is not allowed in this repository" >&2
			exit 1
		fi
		;;
	*)
		# Anything else (is there anything else?)
		echo "*** Update hook: unknown type of update to ref $refname of type $newrev_type" >&2
		exit 1
		;;
esac

# --- Finished
exit 0

================
File: frontend/.git/info/exclude
================
# git ls-files --others --exclude-from=.git/info/exclude
# Lines that start with '#' are comments.
# For a project mostly in C, the following would be a good set of
# exclude patterns (uncomment them if you want to use them):
# *.[oa]
# *~

================
File: frontend/.git/refs/heads/master
================
79dcc937c46bbf4adb5ab4d7d47d01f6ff1644e8

================
File: frontend/.git/COMMIT_EDITMSG
================
Initialize project using Create React App

================
File: frontend/.git/config
================
[core]
	repositoryformatversion = 0
	filemode = true
	bare = false
	logallrefupdates = true
	ignorecase = true
	precomposeunicode = true

================
File: frontend/.git/description
================
Unnamed repository; edit this file 'description' to name the repository.

================
File: frontend/.git/HEAD
================
ref: refs/heads/master

================
File: frontend/public/index.html
================
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" href="%PUBLIC_URL%/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta
      name="description"
      content="Web site created using create-react-app"
    />
    <link rel="apple-touch-icon" href="%PUBLIC_URL%/logo192.png" />
    <!--
      manifest.json provides metadata used when your web app is installed on a
      user's mobile device or desktop. See https://developers.google.com/web/fundamentals/web-app-manifest/
    -->
    <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
    <!--
      Notice the use of %PUBLIC_URL% in the tags above.
      It will be replaced with the URL of the `public` folder during the build.
      Only files inside the `public` folder can be referenced from the HTML.

      Unlike "/favicon.ico" or "favicon.ico", "%PUBLIC_URL%/favicon.ico" will
      work correctly both with client-side routing and a non-root public URL.
      Learn how to configure a non-root public URL by running `npm run build`.
    -->
    <title>React App</title>
  </head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
    <!--
      This HTML file is a template.
      If you open it directly in the browser, you will see an empty page.

      You can add webfonts, meta tags, or analytics to this file.
      The build step will place the bundled scripts into the <body> tag.

      To begin the development, run `npm start` or `yarn start`.
      To create a production bundle, use `npm run build` or `yarn build`.
    -->
  </body>
</html>

================
File: frontend/public/robots.txt
================
# https://www.robotstxt.org/robotstxt.html
User-agent: *
Disallow:

================
File: frontend/src/components/AwardsSearch.js
================
import React, { useState } from 'react';
import axios from 'axios';

const API_URL = 'http://localhost:8000';

function AwardsSearch() {
  const [query, setQuery] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState('');
  const [result, setResult] = useState(null);

  const handleSubmit = async (e) => {
    e.preventDefault();
    
    if (!query.trim()) {
      setError('Please enter a search query');
      return;
    }

    setIsLoading(true);
    setError('');
    setResult(null);

    try {
      const response = await axios.post(`${API_URL}/awards`, {
        query: query
      });
      
      setResult(response.data);
    } catch (err) {
      console.error('Error searching awards:', err);
      setError(err.response?.data?.detail || 'An error occurred while searching awards');
    } finally {
      setIsLoading(false);
    }
  };

  return (
    <div className="search-container">
      <h2>Awards Search</h2>
      <p>Ask questions about Sundt Construction awards and recognition</p>
      
      <form className="search-form" onSubmit={handleSubmit}>
        <input 
          type="text" 
          value={query} 
          onChange={(e) => setQuery(e.target.value)}
          placeholder="E.g., What safety awards has Sundt won?"
        />
        <button type="submit" disabled={isLoading}>
          {isLoading ? 'Searching...' : 'Search'}
        </button>
      </form>

      {error && <div className="error">{error}</div>}

      {isLoading && <div className="loading">Searching for awards...</div>}

      {result && (
        <div className="results-container">
          <div className="response">
            <h3>Response:</h3>
            <p>{result.response}</p>
          </div>

          {result.awards && result.awards.length > 0 && (
            <>
              <h3>Matching Awards ({result.awards.length})</h3>
              <div className="results-list">
                {result.awards.map((award, index) => (
                  <div className="result-card" key={index}>
                    {award.image_url && (
                      <img src={award.image_url} alt={award.title} />
                    )}
                    <div className="result-card-content">
                      <h3>{award.title}</h3>
                      
                      <div className="result-metadata">
                        {award.organization && <p><strong>Organization:</strong> {award.organization}</p>}
                        {award.category && <p><strong>Category:</strong> {award.category}</p>}
                        {award.date && <p><strong>Date:</strong> {award.date}</p>}
                        {award.year && <p><strong>Year:</strong> {award.year}</p>}
                      </div>
                      
                      {award.description && (
                        <p>{award.description.length > 150 
                          ? `${award.description.substring(0, 150)}...` 
                          : award.description}
                        </p>
                      )}
                      
                      {award.url && (
                        <a href={award.url} target="_blank" rel="noopener noreferrer">
                          View Award
                        </a>
                      )}
                    </div>
                  </div>
                ))}
              </div>
            </>
          )}
        </div>
      )}
    </div>
  );
}

export default AwardsSearch;

================
File: frontend/src/components/ProjectsSearch.js
================
import React, { useState } from 'react';
import axios from 'axios';

const API_URL = 'http://localhost:8000';

function ProjectsSearch() {
  const [query, setQuery] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState('');
  const [result, setResult] = useState(null);

  const handleSubmit = async (e) => {
    e.preventDefault();
    
    if (!query.trim()) {
      setError('Please enter a search query');
      return;
    }

    setIsLoading(true);
    setError('');
    setResult(null);

    try {
      const response = await axios.post(`${API_URL}/projects`, {
        query: query
      });
      
      setResult(response.data);
    } catch (err) {
      console.error('Error searching projects:', err);
      setError(err.response?.data?.detail || 'An error occurred while searching projects');
    } finally {
      setIsLoading(false);
    }
  };

  return (
    <div className="search-container">
      <h2>Projects Search</h2>
      <p>Ask questions about Sundt Construction projects</p>
      
      <form className="search-form" onSubmit={handleSubmit}>
        <input 
          type="text" 
          value={query} 
          onChange={(e) => setQuery(e.target.value)}
          placeholder="E.g., Tell me about water treatment projects"
        />
        <button type="submit" disabled={isLoading}>
          {isLoading ? 'Searching...' : 'Search'}
        </button>
      </form>

      {error && <div className="error">{error}</div>}

      {isLoading && <div className="loading">Searching for projects...</div>}

      {result && (
        <div className="results-container">
          <div className="response">
            <h3>Response:</h3>
            <p>{result.response}</p>
          </div>

          {result.projects && result.projects.length > 0 && (
            <>
              <h3>Matching Projects ({result.projects.length})</h3>
              <div className="results-list">
                {result.projects.map((project, index) => (
                  <div className="result-card" key={index}>
                    {project.image_url && (
                      <img src={project.image_url} alt={project.title} />
                    )}
                    <div className="result-card-content">
                      <h3>{project.title}</h3>
                      
                      <div className="result-metadata">
                        {project.location && <p><strong>Location:</strong> {project.location}</p>}
                        {project.client && <p><strong>Client:</strong> {project.client}</p>}
                        {project.value && <p><strong>Value:</strong> {project.value}</p>}
                      </div>
                      
                      {project.description && (
                        <p>{project.description.length > 150 
                          ? `${project.description.substring(0, 150)}...` 
                          : project.description}
                        </p>
                      )}
                      
                      {project.url && (
                        <a href={project.url} target="_blank" rel="noopener noreferrer">
                          View Project
                        </a>
                      )}
                    </div>
                  </div>
                ))}
              </div>
            </>
          )}
        </div>
      )}
    </div>
  );
}

export default ProjectsSearch;

================
File: frontend/src/App.css
================
.App {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
  max-width: 1200px;
  margin: 0 auto;
  padding: 20px;
}

.App-header {
  border-bottom: 1px solid #eaeaea;
  padding-bottom: 20px;
  margin-bottom: 20px;
}

.App-header h1 {
  color: #333;
  margin-bottom: 20px;
}

.tabs {
  display: flex;
  gap: 10px;
}

.tabs button {
  padding: 10px 20px;
  border: none;
  background: #f5f5f5;
  cursor: pointer;
  font-size: 16px;
  border-radius: 4px;
  transition: all 0.2s;
}

.tabs button.active {
  background: #0066cc;
  color: white;
}

.tabs button:hover:not(.active) {
  background: #e0e0e0;
}

main {
  min-height: 70vh;
}

.search-container {
  margin-bottom: 30px;
}

.search-form {
  display: flex;
  gap: 10px;
  margin-bottom: 20px;
}

.search-form input {
  flex: 1;
  padding: 10px;
  font-size: 16px;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.search-form button {
  padding: 10px 20px;
  background: #0066cc;
  color: white;
  border: none;
  border-radius: 4px;
  cursor: pointer;
  font-size: 16px;
}

.search-form button:hover {
  background: #0055aa;
}

.results-container {
  margin-top: 20px;
}

.loading {
  text-align: center;
  padding: 20px;
  color: #666;
}

.response {
  background: #f9f9f9;
  padding: 20px;
  border-radius: 4px;
  margin-bottom: 20px;
  white-space: pre-wrap;
}

.results-list {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
  gap: 20px;
}

.result-card {
  border: 1px solid #eaeaea;
  border-radius: 8px;
  overflow: hidden;
  transition: transform 0.2s, box-shadow 0.2s;
}

.result-card:hover {
  transform: translateY(-5px);
  box-shadow: 0 10px 20px rgba(0,0,0,0.1);
}

.result-card-content {
  padding: 15px;
}

.result-card h3 {
  margin-top: 0;
  color: #0066cc;
}

.result-card img {
  width: 100%;
  height: 180px;
  object-fit: cover;
}

.result-metadata {
  color: #666;
  font-size: 14px;
  margin: 10px 0;
}

.error {
  color: #cc0000;
  background: #ffeeee;
  padding: 15px;
  border-radius: 4px;
  margin-bottom: 20px;
}

footer {
  margin-top: 40px;
  padding-top: 20px;
  border-top: 1px solid #eaeaea;
  text-align: center;
  color: #666;
  font-size: 14px;
}

================
File: frontend/src/App.js
================
import React, { useState } from 'react';
import './App.css';
import ProjectsSearch from './components/ProjectsSearch';
import AwardsSearch from './components/AwardsSearch';

function App() {
  const [activeTab, setActiveTab] = useState('projects');

  return (
    <div className="App">
      <header className="App-header">
        <h1>Sundt Construction RAG System</h1>
        <div className="tabs">
          <button 
            className={activeTab === 'projects' ? 'active' : ''} 
            onClick={() => setActiveTab('projects')}
          >
            Projects
          </button>
          <button 
            className={activeTab === 'awards' ? 'active' : ''} 
            onClick={() => setActiveTab('awards')}
          >
            Awards
          </button>
        </div>
      </header>
      <main>
        {activeTab === 'projects' ? (
          <ProjectsSearch />
        ) : (
          <AwardsSearch />
        )}
      </main>
      <footer>
        <p>Sundt RAG System - Connected to API at http://localhost:8000</p>
      </footer>
    </div>
  );
}

export default App;

================
File: frontend/src/App.test.js
================
import { render, screen } from '@testing-library/react';
import App from './App';

test('renders learn react link', () => {
  render(<App />);
  const linkElement = screen.getByText(/learn react/i);
  expect(linkElement).toBeInTheDocument();
});

================
File: frontend/src/index.css
================
body {
  margin: 0;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  background-color: #f8f9fa;
  color: #333;
}

code {
  font-family: source-code-pro, Menlo, Monaco, Consolas, 'Courier New', monospace;
}

* {
  box-sizing: border-box;
}

a {
  color: #0066cc;
  text-decoration: none;
}

a:hover {
  text-decoration: underline;
}

================
File: frontend/src/index.js
================
import React from 'react';
import ReactDOM from 'react-dom/client';
import './index.css';
import App from './App';

const root = ReactDOM.createRoot(document.getElementById('root'));
root.render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
);

================
File: frontend/src/logo.svg
================
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 841.9 595.3"><g fill="#61DAFB"><path d="M666.3 296.5c0-32.5-40.7-63.3-103.1-82.4 14.4-63.6 8-114.2-20.2-130.4-6.5-3.8-14.1-5.6-22.4-5.6v22.3c4.6 0 8.3.9 11.4 2.6 13.6 7.8 19.5 37.5 14.9 75.7-1.1 9.4-2.9 19.3-5.1 29.4-19.6-4.8-41-8.5-63.5-10.9-13.5-18.5-27.5-35.3-41.6-50 32.6-30.3 63.2-46.9 84-46.9V78c-27.5 0-63.5 19.6-99.9 53.6-36.4-33.8-72.4-53.2-99.9-53.2v22.3c20.7 0 51.4 16.5 84 46.6-14 14.7-28 31.4-41.3 49.9-22.6 2.4-44 6.1-63.6 11-2.3-10-4-19.7-5.2-29-4.7-38.2 1.1-67.9 14.6-75.8 3-1.8 6.9-2.6 11.5-2.6V78.5c-8.4 0-16 1.8-22.6 5.6-28.1 16.2-34.4 66.7-19.9 130.1-62.2 19.2-102.7 49.9-102.7 82.3 0 32.5 40.7 63.3 103.1 82.4-14.4 63.6-8 114.2 20.2 130.4 6.5 3.8 14.1 5.6 22.5 5.6 27.5 0 63.5-19.6 99.9-53.6 36.4 33.8 72.4 53.2 99.9 53.2 8.4 0 16-1.8 22.6-5.6 28.1-16.2 34.4-66.7 19.9-130.1 62-19.1 102.5-49.9 102.5-82.3zm-130.2-66.7c-3.7 12.9-8.3 26.2-13.5 39.5-4.1-8-8.4-16-13.1-24-4.6-8-9.5-15.8-14.4-23.4 14.2 2.1 27.9 4.7 41 7.9zm-45.8 106.5c-7.8 13.5-15.8 26.3-24.1 38.2-14.9 1.3-30 2-45.2 2-15.1 0-30.2-.7-45-1.9-8.3-11.9-16.4-24.6-24.2-38-7.6-13.1-14.5-26.4-20.8-39.8 6.2-13.4 13.2-26.8 20.7-39.9 7.8-13.5 15.8-26.3 24.1-38.2 14.9-1.3 30-2 45.2-2 15.1 0 30.2.7 45 1.9 8.3 11.9 16.4 24.6 24.2 38 7.6 13.1 14.5 26.4 20.8 39.8-6.3 13.4-13.2 26.8-20.7 39.9zm32.3-13c5.4 13.4 10 26.8 13.8 39.8-13.1 3.2-26.9 5.9-41.2 8 4.9-7.7 9.8-15.6 14.4-23.7 4.6-8 8.9-16.1 13-24.1zM421.2 430c-9.3-9.6-18.6-20.3-27.8-32 9 .4 18.2.7 27.5.7 9.4 0 18.7-.2 27.8-.7-9 11.7-18.3 22.4-27.5 32zm-74.4-58.9c-14.2-2.1-27.9-4.7-41-7.9 3.7-12.9 8.3-26.2 13.5-39.5 4.1 8 8.4 16 13.1 24 4.7 8 9.5 15.8 14.4 23.4zM420.7 163c9.3 9.6 18.6 20.3 27.8 32-9-.4-18.2-.7-27.5-.7-9.4 0-18.7.2-27.8.7 9-11.7 18.3-22.4 27.5-32zm-74 58.9c-4.9 7.7-9.8 15.6-14.4 23.7-4.6 8-8.9 16-13 24-5.4-13.4-10-26.8-13.8-39.8 13.1-3.1 26.9-5.8 41.2-7.9zm-90.5 125.2c-35.4-15.1-58.3-34.9-58.3-50.6 0-15.7 22.9-35.6 58.3-50.6 8.6-3.7 18-7 27.7-10.1 5.7 19.6 13.2 40 22.5 60.9-9.2 20.8-16.6 41.1-22.2 60.6-9.9-3.1-19.3-6.5-28-10.2zM310 490c-13.6-7.8-19.5-37.5-14.9-75.7 1.1-9.4 2.9-19.3 5.1-29.4 19.6 4.8 41 8.5 63.5 10.9 13.5 18.5 27.5 35.3 41.6 50-32.6 30.3-63.2 46.9-84 46.9-4.5-.1-8.3-1-11.3-2.7zm237.2-76.2c4.7 38.2-1.1 67.9-14.6 75.8-3 1.8-6.9 2.6-11.5 2.6-20.7 0-51.4-16.5-84-46.6 14-14.7 28-31.4 41.3-49.9 22.6-2.4 44-6.1 63.6-11 2.3 10.1 4.1 19.8 5.2 29.1zm38.5-66.7c-8.6 3.7-18 7-27.7 10.1-5.7-19.6-13.2-40-22.5-60.9 9.2-20.8 16.6-41.1 22.2-60.6 9.9 3.1 19.3 6.5 28.1 10.2 35.4 15.1 58.3 34.9 58.3 50.6-.1 15.7-23 35.6-58.4 50.6zM320.8 78.4z"/><circle cx="420.9" cy="296.5" r="45.7"/><path d="M520.5 78.1z"/></g></svg>

================
File: frontend/src/reportWebVitals.js
================
const reportWebVitals = onPerfEntry => {
  if (onPerfEntry && onPerfEntry instanceof Function) {
    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {
      getCLS(onPerfEntry);
      getFID(onPerfEntry);
      getFCP(onPerfEntry);
      getLCP(onPerfEntry);
      getTTFB(onPerfEntry);
    });
  }
};

export default reportWebVitals;

================
File: frontend/src/setupTests.js
================
// jest-dom adds custom jest matchers for asserting on DOM nodes.
// allows you to do things like:
// expect(element).toHaveTextContent(/react/i)
// learn more: https://github.com/testing-library/jest-dom
import '@testing-library/jest-dom';

================
File: frontend/.gitignore
================
# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
/node_modules
/.pnp
.pnp.js

# testing
/coverage

# production
/build

# misc
.DS_Store
.env.local
.env.development.local
.env.test.local
.env.production.local

npm-debug.log*
yarn-debug.log*
yarn-error.log*

================
File: frontend/README.md
================
# Getting Started with Create React App

This project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).

## Available Scripts

In the project directory, you can run:

### `npm start`

Runs the app in the development mode.\
Open [http://localhost:3000](http://localhost:3000) to view it in your browser.

The page will reload when you make changes.\
You may also see any lint errors in the console.

### `npm test`

Launches the test runner in the interactive watch mode.\
See the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.

### `npm run build`

Builds the app for production to the `build` folder.\
It correctly bundles React in production mode and optimizes the build for the best performance.

The build is minified and the filenames include the hashes.\
Your app is ready to be deployed!

See the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.

### `npm run eject`

**Note: this is a one-way operation. Once you `eject`, you can't go back!**

If you aren't satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.

Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you're on your own.

You don't have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn't feel obligated to use this feature. However we understand that this tool wouldn't be useful if you couldn't customize it when you are ready for it.

## Learn More

You can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).

To learn React, check out the [React documentation](https://reactjs.org/).

### Code Splitting

This section has moved here: [https://facebook.github.io/create-react-app/docs/code-splitting](https://facebook.github.io/create-react-app/docs/code-splitting)

### Analyzing the Bundle Size

This section has moved here: [https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size](https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size)

### Making a Progressive Web App

This section has moved here: [https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app](https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app)

### Advanced Configuration

This section has moved here: [https://facebook.github.io/create-react-app/docs/advanced-configuration](https://facebook.github.io/create-react-app/docs/advanced-configuration)

### Deployment

This section has moved here: [https://facebook.github.io/create-react-app/docs/deployment](https://facebook.github.io/create-react-app/docs/deployment)

### `npm run build` fails to minify

This section has moved here: [https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify](https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify)

================
File: .gitignore
================
# Python bytecode
__pycache__/
*.py[cod]
*$py.class
*.so
.Python

# Virtual environments
venv/
env/
ENV/
.env/
.venv/
env.bak/
venv.bak/
.conda/

# Environment variables and secrets
.env
*.env
.secret*

# Data files and directories
data/
*.json
*.csv
*.xlsx
*.xls
*.db
*.sqlite
*.sqlite3

# Logs and temporary files
logs/
*.log
*.log.*
log/
tmp/
temp/
.DS_Store
Thumbs.db
.ipynb_checkpoints/
*.swp
*.bak
*~

# Node/JavaScript/React
node_modules/
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnp/
.pnp.js
.npm
.yarn/cache
.yarn/unplugged
.yarn/build-state.yml
.yarn/install-state.gz
frontend/build/
frontend/dist/
frontend/.cache/
coverage/
*.lcov
.nyc_output/

# Testing
.coverage
htmlcov/
.tox/
.nox/
.pytest_cache/
nosetests.xml
coverage.xml

# Distribution / packaging
dist/
build/
*.egg-info/
.eggs/
*.egg
wheels/
*.whl

# Editors and IDEs
.idea/
.vscode/
*.sublime-project
*.sublime-workspace
.eclipse/
.settings/
.classpath
.project
*.code-workspace
.history/

# Tool-specific
.repopack/
repopack-output.txt

# Exclude from ignore (keep these files)
!requirements.txt
!.gitignore
!README.md
!LICENSE
!setup.py
!pyproject.toml
